\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf {Defining the data integration strategy: choosing the common coordinate framework}. Schematic representation of (a) Horizontal integration, when features act as anchors (b) Vertical integration, when cells act as anchors (c) Diagonal integration, when no anchors exist. \relax }}{11}{figure.caption.6}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces The quantity $\mathcal {L}({\bf X})$ provides a lower bound on the true log marginal likelihood $\qopname \relax o{log}p({\bf Y})$, with the difference being given by the Kullback-Leibler divergence ${\rm KL}(q||p)$ between the variational distribution $q({\bf X})$ and the true posterior $p({\bf X}|{\bf Y})$\relax }}{38}{figure.caption.17}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Illustrative example of sampling from a true posterior distribution (blue) versus a fitted mean-field varaitional distribution (red) in a model with two (correlated) unobserved variables. The mean-field approximation wrongly assumes that the unobserved variables are independent.\relax }}{39}{figure.caption.18}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Illustrative comparison of Variational inference and Expectation Propagation. Shown is the (a) Density and (b) Variance of the true posterior distribution $p({\bf X}|{\bf Y})$ (grey), the variational distribution (orange) and the expectation propagation distribution (green).\relax }}{41}{figure.caption.19}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Maximizing the variance in the principal component space is equivalent to minimizing the data reconstruction error}}{44}{figure.caption.20}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.5}{\ignorespaces Graphical model for probabilistic PCA. The latent variables are modelled as random variables, whereas the weights and the noise are modelled as deterministic parameters.\relax }}{45}{figure.caption.21}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.6}{\ignorespaces Graphical model for Bayesian Factor Analysis. All unobserved variables are modelled as random variables.\relax }}{47}{figure.caption.22}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.7}{\ignorespaces Visualisation of the sparsity-inducing Automatic Relevance Determination prior\relax }}{48}{figure.caption.23}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.8}{\ignorespaces Graphical model for Bayesian sparse Factor Analysis. A double sparsity-inducing prior is used on the weights: an ARD prior to prune inactive factors and a spike-and-slab prior to inactive individual features within the active factors.\relax }}{49}{figure.caption.24}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.9}{\ignorespaces Graphical model for probabilistic Canonical Correlation Analysis\relax }}{51}{figure.caption.25}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.10}{\ignorespaces Graphical model for Bayesian Canonical Correlation Analysis\relax }}{52}{figure.caption.26}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.11}{\ignorespaces Graphical model for Bayesian Group Factor Analysis\relax }}{54}{figure.caption.27}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.12}{\ignorespaces Graphical model for MOFA. The white circles represent hidden variables that are infered by the model, whereas the grey circles represent the observed variables. There are a total of four plates, each one representing a dimension of the model: $M$ for the number of views, $N$ for the number of samples, $K$ for the number of factors and $D_m$ for the number of features in the $m$-th view. The use of transparency in the top left nodes is intentional and becomes clear in Chapter 4.\relax }}{57}{figure.caption.28}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.13}{\ignorespaces \textbf {MOFA overview}. The model takes $M$ data matrices as input (${\bf Y}^1, \cdots , {\bf Y}^M$), one or more from each data modality, with co-occurrent samples but features that are not necessarily related and can differ in numbers. MOFA decomposes these matrices into a matrix of factors (${\bf Z}$) and $M$ weight matrices, one for each data modality (${\bf W}^1, \cdots , {\bf W}^M$). White cells in the weight matrices correspond to zeros, i.e. inactive features, whereas the cross symbol in the data matrices denotes missing values. The trained MOFA model can be queried for different downstream analyses.\relax }}{59}{figure.caption.29}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.14}{\ignorespaces \textbf {Model selection and robustness analysis in MOFA}. The left plot the log ELBO (y-axis) for 25 model instances (x-axis). The arrow indicates the model with the highest ELBO that would be selected for downstream analysis. The right plot displays the absolute value of the Pearson correlation coefficient between pairwise combinations of all factors across the 25 model instances. A block-diagonal matrix indicates that factors are robustly estimated regardless of the initialisation.\relax }}{60}{figure.caption.30}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.15}{\ignorespaces \textbf {Training curve for the number of active factors across 25 different model instances}. The y-axis displays the number of active factors. The x-axis displays the iteration number. Different lines denote different model instances.\relax }}{61}{figure.caption.31}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.16}{\ignorespaces Training curve for two diffent instances of MOFA with random initialisations. The y-axis displays the log of the ELBO, with higher values indicating a better fit. The x-axis displays the iteration number. The horizontal dash lines mark the value of the ELBO upon convergence. \relax }}{61}{figure.caption.32}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.17}{\ignorespaces \textbf {Assessing the ability to recover simulated factors}. In all plots the y-axis displays the number of infered factors. (a) x-axis displays the number of true factors, and boxplots summarise the distribution across 10 model instances. For (b-d) the true number of factors was set to $K=10$ and each bar corresponds to a different model instance. (b) x-axis displays the number of features, (c) x-axis displays the number of views, (d) x-axis displays fraction of missing values. \relax }}{66}{figure.caption.34}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.18}{\ignorespaces \textbf {Evaluating the ability to recover differential factor activity across views}. (a) The true activity pattern, with factors sampled to display differential activity across views. (b) Percentage of variance explained for each factor in each view, for MOFA and iCluster \cite {Mo2013}.\relax }}{67}{figure.caption.35}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.19}{\ignorespaces \textbf {Assessing the sparsity priors on the weights}. The plot shows the empirical cumulative density function of the weights for an arbitrary factor in a single view. The weights were simulated with a sparsity level of $\theta _k^m=0.5$ (50\% of active features.) \relax }}{68}{figure.caption.36}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.20}{\ignorespaces \textbf {Validation of the non-Gaussian likelihood models using simulated data}. (a-d) Comparison of Poisson and Gaussian likelihood models applied to count data. (e-h) Comparison of Bernoulli and Gaussian likelihood models applied to binary data. (a,e) The y-axis displays the ELBO for each model instance (x-axis). (b,f) The y-axis displays the mean reconstruction error for each model instance (x-axis). (c,g) The y-axis displays the number of estimated factrors for each model instance (x-axis). The horizontal dashed line marks the true number of factors $K=10$. (d,h) Distribution of reconstructed data. Plotted are the expected values of the inferred posterior distributions, not samples from the corresponding posteriors. This is why reconstructed measurements are continuous and not discrete. \relax }}{69}{figure.caption.37}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.21}{\ignorespaces \textbf {Evaluation of scalability in MOFA}. Shown is the time required for convergence (y-axis, in minutes). The x-axis displays the value of the dimension that was tested, either number of factors ($K$), number of features ($D$), number of samples ($N$) and number of views ($M$). Baseline parameters were $M=3, K=10, D=1000, N=100$. Each line represents a different model, GFA (red), MOFA (blue) and iCluster (green). Default convergence criteria where used for all methods. Each dot displays the average time across 10 trials with error bars denoting the standard deviation. iCluster is only shown for one value as all other settings required more than 200min for convergence. \relax }}{70}{figure.caption.38}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.22}{\ignorespaces \textbf {Application of MOFA to a study of chronic lymphocytic leukaemia. Model overview.} (a) Data overview. Assays are shown in different rows ($D$ = number of features) and samples ($N$) in columns, with missing samples shown using grey bars. Notice that some samples are missing entire assays. (b) Variance explained (\%) by each Factor in each assay. (c) Total variance explained (\%) for each assay by all factors. \relax }}{72}{figure.caption.39}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.23}{\ignorespaces \textbf {Visualisation of the genetic signature underlying Factor 1 and 2} (a) Absolute weights of the top features of Factors 1 and 2 in the Mutations data. (b) Visualization of samples using Factors 1 and 2. The colours denote the IGHV status of the tumours; symbol shape and colour tone indicate chromosome 12 trisomy status. \relax }}{73}{figure.caption.40}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.24}{\ignorespaces \textbf {Characterization of MOFA Factor 1 as IGHV status.} (a) Beeswarm plot with Factor 1 values for each sample with colours corresponding to three groups found by 3-means clustering with low factor values (LZ), intermediate factor values (IZ) and high factor values (HZ). (b) Absolute weights for the genes with the largest absolute weights in the mRNA data. Plus or minus symbols on the right indicate the sign of the loading. Genes highlighted in orange were previously described as prognostic markers in CLL and associated with IGHV status (Vasconcelos et al, 2005; Maloum et al, 2009; Trojani et al, 2012; Morabito et al, 2015; Plesingerova et al, 2017). (c) Heatmap of gene expression values for genes with the largest weights as in (b). (d) Absolute weights of the drugs with the largest weights, annotated by target category. (e) Drug response curves for two of the drugs with top weights, stratified by the clusters as in (a). \relax }}{74}{figure.caption.41}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.25}{\ignorespaces \textbf {Characterization of Factor 5 in the CLL cohort as oxidative stress response.} (a) Beeswarm plot of Factor 5. Colours denote the expression of TNF, an inflammatory stress marker that is present among the top mRNA weights. (b) Gene set enrichment analysis for the top Reactome pathways. Displayed are the top pathways with the strongest enrichment in the mRNA weights. (c) Heatmap of mRNA expression values for representative genes among the top weights. Samples are ordered by their Factor 5 values. (d) Weights for the top drugs, annotated by target category. (e) Heatmap of drug response values for the top three drugs. \relax }}{76}{figure.caption.42}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.26}{\ignorespaces \textbf {Association analysis between MOFA factors and clinical putcome.} (a) Association of MOFA factors to time to next treatment using a univariate Cox regression with N = 174 samples (96 of which are uncensored cases) and p-values based on the Wald statistic. Error bars denote 95\% confidence intervals. Numbers on the right denote p-values for each predictor. (b) Kaplan-Meier plots measuring time to next treatment for the individual MOFA factors. The cut-points on each factor were chosen using maximally selected rank statistics, and p-values were calculated using a log-rank test on the resulting groups. (c) Prediction accuracy of time to treatment for N = 174 patients using multivariate Cox regression trained with the 10 MOFA factors, as well using the first 10 principal components applied to single data modalities and the full data set. Shown are average values of Harrell's C-index from fivefold cross-validation. Error bars denote standard error of the mean. \relax }}{77}{figure.caption.43}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.27}{\ignorespaces \textbf {Evaluation of imputation performance in the drug response assay.} The y-axis shows the mean-squared error (MSE) across 15 trials for increasing fractions of missing data (x-axis). Two experiments were considered: (a) values missing at random and (b) entire assays missing at random. Each point displays the mean across all trials and the error bars depict the corresponding standard deviations.\relax }}{78}{figure.caption.44}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.28}{\ignorespaces \textbf {MOFA recovers a differentiation process from a single-cell multi-omics data set.} (a) Overview of the data modalities. Rows indicate number of features ($D$) and columns indicate number of samples ($N$). Grey bars denote missing samples. (b) Fraction of variance explained per factor (column) and view (row). (c) Cumulative fraction of variance explained per view (across all factors). (d) mRNA weights of Factor 1 (bottom) and Factor 2 (top). The genes that are labelled are known markers of pluripotency (for Factor 1) or differentiation (for Factor 2). (e) Scatter plot of Factor 1 (x-axis) against Factor 2 (y-axis). Cells are colored based on the culture condition. Grey arrow illustrates the differentiation trajectory from a naive pluripotency state to a differentiated state. \relax }}{80}{figure.caption.45}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.8}{\ignorespaces \textbf {Multi-Omics Factor Analysis (MOFA): model overview and illustration of downstream analysis.} (a) Model overview: MOFA takes as input one or more data modalities (${\bf Y}$), extracted from the same cells. MOFA decomposes these matrices into a matrix of latent factors (${\bf Z}$) and a set of feature weight matrices (${\bf W}$), one for each data modality. (b) Downstream analysis: the fitted MOFA model can be queried for different downstream analyses, including (i) variance decomposition, assessing the proportion of variance ($R^2$) explained by each factor in each data modality, (ii) factor interpretation based on the inspection of weights and gene set enrichment analysis, (iii) visualization of the cells in the factor space. \relax }}{93}{figure.caption.53}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.9}{\ignorespaces Venn diagrams showing overlap of peak calls for each lineage-specific histone mark, for distal H3K27ac (left) and all H3K4me3 (right). The figure shows that distal H3K27ac peaks (putative enhancer \cite {Creyghton2010}) have moderate levels of overlap between the three germ layers. In contrast, H3K4me3 peaks (active transcription start sites \cite {Liang2004}) are similar between the three germ layers. \relax }}{94}{figure.caption.54}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.19}{\ignorespaces \textbf {Schematic illustration of the hierarchical model for the epigenetic dynamics of germ layer commitment.} Illustration designed by Veronique Juvin from SciArtWork. \relax }}{104}{figure.caption.64}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf {Illustration of the problem of using the Euclidean norm as a distance measure between parameters of probability distributions}. In both plots, the red and blue distributions are separated by the same Euclidean distance of 100. Yet, the distance in probability space between the two distributions is higher in the right. \relax }}{116}{figure.caption.71}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf {Graphical model for a general probabilistic model where unobserved variables are classified as \textit {global} and \textit {local}.} The dashed line indicates that the connection between \textit {global} and \textit {local} variables is optional, not used in the MOFA model. \relax }}{118}{figure.caption.72}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf {Graphical model for MOFA+.} The white circles represent hidden variables that are inferred by the model, whereas the grey circles represent the observed variables. There are a total of five plates, each one representing a dimension of the model: $M$ for the number of views, $G$ for the number of groups, $K$ for the number of factors, $D_m$ for the number of features in the $m$-th view and $N_g$ for the number of samples in the $g$-th group. \relax }}{126}{figure.caption.74}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
