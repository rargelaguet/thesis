\section{Limitations and open perspectives}

MOFA solves important challenges for the integrative analysis of (single-cell) multi-omics datasets. Yet, the model is not free of limitations and there are open possibilities for future research:

\begin{itemize}

	\item \textbf{Linearity}: this is an assumption that is critical for obtaining interpretable feature weights. Nonetheless, there is a trade-off between explanatory power and interpretability\cite{Kuhn}. Non-linear approaches, including deep neural networks or variational autoencoders have shown promising results when it comes to dimensionality reduction \cite{Lin2017,Ding2018,Lopez2018}, batch correction\cite{Lopez2018}, denoising \cite{Eraslan2019} or imputation \cite{Lin2016}. Interestingly, very few multi-view factor analysis models exist that incorporate flexible non-linear assumptions, making it an interesting line of research to explore.

	\item \textbf{Scalability}: the size of biological datasets is rapidly increasing, particularly in the field of single cell sequencing \cite{Svensson2018,Cao2019}. When comparing the inference framework to previous methods that make use of sampling-based MCMC approaches, the variatonal framework implemented in MOFA yields a vast improvement in scalability. Yet, in its vanilla form, variational inference also becomes prohibitively slow with very large datasets \cite{Hoffman2013,Blei2016,Hoffman2014}. This has been recently addressed by a reformulation of the variational inference problem in terms of a gradient descent optimisation problem, which enables the full machinery of stochastic inference to be applied in the context of Bayesian inference. This line of research is followed in Chapter 5, with the development of a stochastic version of the variational inference algorithm.

	\item \textbf{Generalisations to multi-group structures}: the sparsity assumptions in MOFA are based on the principle that features are structured into non-overlapping views. As such, the activity of the latent factors is also expected to be structured, so that different factors explain variability in different subsets of views (\Cref{fig:MOFA}). Following the same logic, many studies contain structured samples, as either multiple experiments or conditions. A simple generalisation of MOFA would be to intuitively break the assumption of independent samples and introduce an additional prior that captures the group structure at the sample level. This line of research is followed in Chapter 5.

	% \item \textbf{Tailored likelihoods for single-cell assays}: MOFA enables the modular extension to arbitrary non-Gaussian likelihoods, provided that they can be locally bounded and integrated into the variational framework (see \Cref{section:mofa_ngaussian}). New likelihood models such as zero-inflated negative binomial distributions \cite{Risso2018} could make MOFA more suited to the analysis of single-cell data.

	\item \textbf{Bayesian treatment of predictions}: in the current implementation of MOFA, only the point estimates for the posterior distributions are used in the downstream analysis. While convienient for most operations, this ignores the uncertainity associated with the point estimates, which is a major strength of Bayesian modelling. Future extensions could attempt a more comprehensive Bayesian treatment that propagates uncertainity in the downstream analyses, mainly when it comes to making predictions and imputation \cite{Gelman2013}.

	\item \textbf{Incorporation of prior information}: an unsupervised approach is appealing for discovering the principal axes of variation, but sometimes this can yield challenges in the interpretation of factors. Future extensions could exploit the rich information encoded in gene set ontologies, similar to the methodology proposed in \cite{Buettner2017}.

	\item \textbf{View imbalance}: a property of MOFA is that the number of features can influence the contribution of a data modality to the latent space, such that bigger views tend to contribute more to the factors. This is because the objective function (the evidence lower bound, ELBO) does not weight the different data modalities according to their number of features. This is not necessarily a problem, as we have demonstrated in \Cref{section:mofa_cll}, where we extracted meaningful signal from small data modalities. In general, however, the signal that can be extracted from small data modalities will depend on the degree of structure within the dataset, the levels of noise and on how strong the feature imbalance is between data modalities. In practice we suggest users to try balance the number of features by subsetting highly variable features in the larger views. An alternative option would be to weight the contribution of each view on the ELBO, such that small views have a relatively higher contribution than large views. This is however a heuristic that does not arise from the Bayesian generative model and there would be no theoretical guarantees about its behaviour.

\end{itemize}

