@ARTICLE{Minka2001,
       author = {{Minka}, Thomas P.},
        title = "{Expectation Propagation for approximate Bayesian inference}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = "2013",
        month = "Jan",
          eid = {arXiv:1301.2294},
        pages = {arXiv:1301.2294},
archivePrefix = {arXiv},
       eprint = {1301.2294}

}



@article{Cao2019,
	Abstract = {Mammalian organogenesis is a remarkable process. Within a short timeframe, the cells of the three germ layers transform into an embryo that includes most of the major internal and external organs. Here we investigate the transcriptional dynamics of mouse organogenesis at single-cell resolution. Using single-cell combinatorial indexing, we profiled the transcriptomes of around 2 million cells derived from 61 embryos staged between 9.5 and 13.5 days of gestation, in a single experiment. The resulting `mouse organogenesis cell atlas'(MOCA) provides a global view of developmental processes during this critical window. We use Monocle 3 to identify hundreds of cell types and 56 trajectories, many of which are detected only because of the depth of cellular coverage, and collectively define thousands of corresponding marker genes. We explore the dynamics of gene expression within cell types and trajectories over time, including focused analyses of the apical ectodermal ridge, limb mesenchyme and skeletal muscle.},
	Author = {Cao, Junyue and Spielmann, Malte and Qiu, Xiaojie and Huang, Xingfan and Ibrahim, Daniel M. and Hill, Andrew J. and Zhang, Fan and Mundlos, Stefan and Christiansen, Lena and Steemers, Frank J. and Trapnell, Cole and Shendure, Jay},
	Doi = {10.1038/s41586-019-0969-x},
	Id = {Cao2019},
	Isbn = {1476-4687},
	Journal = {Nature},
	Number = {7745},
	Pages = {496--502},
	Title = {The single-cell transcriptional landscape of mammalian organogenesis},
	Volume = {566},
	Year = {2019}
}



@ARTICLE{Martens2014,
	author = {{Martens}, James},
	title = "{New insights and perspectives on the natural gradient method}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = "2014",
	month = "Dec",
	eid = {arXiv:1412.1193},
	pages = {arXiv:1412.1193},
	archivePrefix = {arXiv},
	eprint = {1412.1193},
	primaryClass = {cs.LG}
}


@misc{Kristiadi2019,
	author = {Kristiadi, Agustinus},
	title = {Natural Gradient Descent},
	type = {Blog},
	number = {January 23},
	year = {2019},
	howpublished = {\url{https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/}}
}
@incollection{Platt2008,
	title = {Fast Variational Inference for Large-scale Internet Diagnosis},
	author = {Emre Kiciman and David Maltz and John C. Platt},
	booktitle = {Advances in Neural Information Processing Systems 20},
	editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
	pages = {1169--1176},
	year = {2008},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3292-fast-variational-inference-for-large-scale-internet-diagnosis.pdf}
}


@article{Ranganath2014,
	author = {{Ranganath}, Rajesh and {Gerrish}, Sean and {Blei}, David M.},
	title = "{Black Box Variational Inference}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Statistics - Computation, Statistics - Methodology},
	year = 2013,
	month = Dec,
	eid = {arXiv:1401.0118},
	pages = {arXiv:1401.0118},
	archivePrefix = {arXiv},
	eprint = {1401.0118},
	primaryClass = {stat.ML}
}



@book{Spall2003,
	title     = "Introduction to stochastic search and optimization: estimation, simulation, and control",
	author    = "Spall, James C",
	publisher = "J. Wiley",
	year      =  2003,
	address   = "Hoboken, N.J"
}


@book{Murphy,
	author = {Murphy, Kevin P.},
	title = {Machine Learning: A Probabilistic Perspective},
	year = {2012},
	isbn = {0262018020, 9780262018029},
	publisher = {The MIT Press},
}


@inproceedings{Ranganath2013,
	author = {Ranganath, Rajesh and Wang, Chong and Blei, David M. and Xing, Eric P.},
	title = {An Adaptive Learning Rate for Stochastic Variational Inference},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	series = {ICML'13},
	year = {2013},
	pages = {II-298--II-306},
	url = {http://dl.acm.org/citation.cfm?id=3042817.3042927},
	publisher = {JMLR.org},
}

@article{Saul1996,
	author = {{Saul}, L.~K. and {Jaakkola}, T. and {Jordan}, M.~I.},
	title = "{Mean Field Theory for Sigmoid Belief Networks}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Artificial Intelligence},
	year = 1996,
	month = Feb,
	eid = {cs/9603102},
	pages = {cs/9603102},
	archivePrefix = {arXiv},
	eprint = {cs/9603102}
}


@article{Zhao2009,
	title = "A note on variational Bayesian factor analysis",
	journal = "Neural Networks",
	volume = "22",
	number = "7",
	pages = "988 - 997",
	year = "2009",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/j.neunet.2008.11.002",
	url = "http://www.sciencedirect.com/science/article/pii/S0893608008002670",
	author = "Jian-hua Zhao and Philip L.H. Yu"
}


@article{Svensson2018,
	Author = {Svensson, Valentine and Vento-Tormo, Roser and Teichmann, Sarah A},
	Date = {2018/03/01/online},
	Day = {01},
	Journal = {Nature Protocols},
	Month = {03},
	Pages = {599 EP  -},
	Publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
	Title = {Exponential scaling of single-cell RNA-seq in the past decade},
	Url = {https://doi.org/10.1038/nprot.2017.149},
	Volume = {13},
	Year = {2018}
}


@incollection{Mackay1996,
	title={Bayesian methods for backpropagation networks},
	author={MacKay, David JC},
	booktitle={Models of neural networks III},
	pages={211--254},
	year={1996},
	publisher={Springer}
}


@article{Mitchell1988,
	title={Bayesian variable selection in linear regression},
	author={Mitchell, Toby J and Beauchamp, John J},
	journal={Journal of the American Statistical Association},
	volume={83},
	number={404},
	pages={1023--1032},
	year={1988},
	publisher={Taylor \& Francis}
}


@article{Gao2013,
	author = {{Gao}, Chuan and {Brown}, Christopher D and {Engelhardt}, Barbara E},
	title = "{A latent factor model with a mixture of sparse and dense factors to model gene expression data with confounding effects}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Applications, Quantitative Biology - Genomics},
	year = 2013,
	eid = {arXiv:1310.4792},
	pages = {arXiv:1310.4792},
	archivePrefix = {arXiv},
	eprint = {1310.4792},
	primaryClass = {stat.AP},
	adsurl = {https://ui.adsabs.harvard.edu/\#abs/2013arXiv1310.4792G},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{Argelaguet2018,
	author = {Argelaguet, R. and Velten, B. and Arnol, D. and Dietrich, S. and Zenz, T. and Marioni, J. C. and Buettner, F. and Huber, W. and Stegle, O.},
	title = {Multi-Omics Factor Analysis-a framework for unsupervised integration of multi-omics data sets},
	journal = {Mol Syst Biol},
	volume = {14},
	number = {6},
	pages = {e8124},
	ISSN = {1744-4292 (Electronic) 1744-4292 (Linking)},
	DOI = {10.15252/msb.20178124},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/29925568},
	year = {2018},
	type = {Journal Article}
}


@thesis{Beal2003,
	author       = {JM Beal},
	title        = {Variational algorithms for approximate bayesian inference},
	school       = {University College London},
	year         = {2003}
}


@article{Jordan1999,
  title    = "An Introduction to Variational Methods for Graphical Models",
  author   = "Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S
              and Saul, Lawrence K",
  journal  = "Mach. Learn.",
  volume   =  37,
  number   =  2,
  pages    = "183--233",
  month    =  nov,
  year     =  1999
}


@article{Amari1998,
  title     = "Natural Gradient Works Efficiently in Learning",
  author    = "Amari, Shun-Ichi",
  journal   = "Neural Comput.",
  publisher = "MIT Press",
  volume    =  10,
  number    =  2,
  pages     = "251--276",
  month     =  feb,
  year      =  1998
}


@article{Robbins-Monro1951,
	title     = "A stochastic approximation method",
	author    = "Robbins, H. and Monro, S.",
	journal   = "The Annals of Mathematical Statistics",
	volume    =  22,
	number    =  3,
	pages     = "400â€“407",
	year      =  1951
}


@article{Emtiyaz2018,
	author = {{Emtiyaz Khan}, Mohammad and {Nielsen}, Didrik},
	title = "{Fast yet Simple Natural-Gradient Descent for Variational Inference in Complex Models}",
	journal = {arXiv e-prints},
	year = 2018,
	month = Jul,
	eid = {arXiv:1807.04489},
	pages = {arXiv:1807.04489},
	archivePrefix = {arXiv},
	eprint = {1807.04489},
	primaryClass = {stat.ML}
}



@article{Hoffman2012,
	author = {{Hoffman}, Matt and {Blei}, David M. and {Wang}, Chong and {Paisley}, John},
	title = "{Stochastic Variational Inference}",
	journal = {arXiv e-prints},
	year = 2012,
	month = Jun,
	eid = {arXiv:1206.7051},
	pages = {arXiv:1206.7051},
	eprint = {1206.7051}
}


@inproceedings{Seeger2012,
	title={Fast variational Bayesian inference for non-conjugate matrix factorization models},
	author={Seeger, Matthias and Bouchard, Guillaume},
	booktitle={Artificial Intelligence and Statistics},
	pages={1012--1018},
	year={2012}
}


@article{Jaakkola2000,
	title={Bayesian parameter estimation via variational methods},
	author={Jaakkola, Tommi S and Jordan, Michael I},
	journal={Statistics and Computing},
	volume={10},
	number={1},
	pages={25--37},
	year={2000},
	publisher={Springer}
}


@inproceedings{Titsias2011,
	title={Spike and slab variational inference for multi-task and multiple kernel learning},
	author={Titsias, Michalis K and L{\'a}zaro-Gredilla, Miguel},
	booktitle={Advances in neural information processing systems},
	pages={2339--2347},
	year={2011}
}


@article{Bishop,
	title={Pattern recognition},
	author={Bishop, Christopher M},
	journal={Machine Learning},
	volume={128},
	pages={1--58},
	year={2006}
}


@article{bunte2016sparse,
	title={Sparse group factor analysis for biclustering of multiple data sources},
	author={Bunte, Kerstin and Lepp{\"a}aho, Eemeli and Saarinen, Inka and Kaski, Samuel},
	journal={Bioinformatics},
	volume={32},
	number={16},
	pages={2457--2463},
	year={2016},
	publisher={Oxford Univ Press}
}



@inproceedings{cupy2017,
	author       = "Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman",
	title        = "CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations",
	booktitle    = "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)",
	year         = "2017",
	url          = "http://learningsys.org/nips17/assets/papers/paper_16.pdf"
}


@article{Zhang2017,
	author = {{Zhang}, Cheng and {Butepage}, Judith and {Kjellstrom}, Hedvig and
	{Mandt}, Stephan},
	title = "{Advances in Variational Inference}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = 2017,
	month = Nov,
	eid = {arXiv:1711.05597},
	pages = {arXiv:1711.05597},
	archivePrefix = {arXiv},
	eprint = {1711.05597}
}


@article{Hoffman2014,
	author = {{Hoffman}, Matthew D. and {Blei}, David M.},
	title = "{Structured Stochastic Variational Inference}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning},
	year = 2014,
	month = Apr,
	eid = {arXiv:1404.4114},
	pages = {arXiv:1404.4114},
	archivePrefix = {arXiv},
	eprint = {1404.4114}
}


@article{Blei2016,
	author = {{Blei}, David M. and {Kucukelbir}, Alp and {McAuliffe}, Jon D.},
	title = "{Variational Inference: A Review for Statisticians}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Computation, Computer Science - Machine Learning, Statistics - Machine Learning},
	year = 2016,
	month = Jan,
	eid = {arXiv:1601.00670},
	pages = {arXiv:1601.00670},
	archivePrefix = {arXiv},
	eprint = {1601.00670},
	primaryClass = {stat.CO}
}

@PHDTHESIS{Hore2015-thesis,
  title  = "Latent Variable Models for Analysing Multidimensional Gene Expression Data",
  author = "Hore, Victoria",
  year   =  2015,
  school = "University of Oxford"
}


@ARTICLE{Stigler2008,
       author = {{Stigler}, Stephen M.},
        title = "{The Epic Story of Maximum Likelihood}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = "2008",
        month = "Apr",
          eid = {arXiv:0804.2996},
        pages = {arXiv:0804.2996},
archivePrefix = {arXiv},
       eprint = {0804.2996},
 primaryClass = {stat.ME},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2008arXiv0804.2996S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@book{Hastie,
 author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
 title = {Statistical Learning with Sparsity: The Lasso and Generalizations},
 year = {2015},
 isbn = {1498712169, 9781498712163},
 publisher = {Chapman \& Hall/CRC},
} 

@article{Bayes1763,
author = {Thomas Bayes  and null Price },
title = {LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S},
journal = {Philosophical Transactions of the Royal Society of London},
volume = {53},
number = {},
pages = {370-418},
year = {1763},
doi = {10.1098/rstl.1763.0053},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstl.1763.0053},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1763.0053}
}





% @book{Raiffa1961,
%   title={Applied statistical decision theory},
%   author={Ra{\"\i}ffa, H. and Schlaifer, R.},
%   isbn={9780875840178},
%   lccn={60011282},
%   series={Studies in managerial economics},
%   url={https://books.google.co.uk/books?id=wPBLAAAAMAAJ},
%   year={1961},
%   publisher={Division of Research, Graduate School of Business Adminitration, Harvard University}
% }


@misc{Gelman2013,
    author = {Gelman, Andrew and Carlin, John and Stern, Hal and Dunson, David and Vehtari, Aki and Rubin, Donald},
    day = {01},
    edition = {Third},
    howpublished = {Hardcover},
    isbn = {1439840954},
    keywords = {bayes},
    month = nov,
    publisher = {Chapman and Hall/CRC},
    title = {Bayesian Data Analysis, Third Edition (Chapman \& {Hall/CRC} Texts in Statistical Science)},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/1439840954},
    year = {2013}
}




@article{Jaynes1968,
	Author = {E.  T.  Jaynes},
	Journal = {IEEE Transactions on Systems Science and Cybernetics},
	Number = {3},
	Pages = {227--241},
	Title = {Prior Probabilities},
	Volume = {4},
	Year = {1968}}



@article{Craiu2014,
	Author = {Craiu, Radu V. and Rosenthal, Jeffrey S.},
	Journal = {Annual Review of Statistics and Its Application},
	Month = {2019/03/14},
	Number = {1},
	Pages = {179--201},
	Title = {Bayesian Computation Via Markov Chain Monte Carlo},
	Volume = {1},
	Year = {2014}}



@article{Rue2009,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/40247579},
 abstract = {Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
 author = {HÃ¥vard Rue and Sara Martino and Nicolas Chopin},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {319--392},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations},
 volume = {71},
 year = {2009}
}


@article{Carbonetto2012,
author = "Carbonetto, Peter and Stephens, Matthew",
doi = "10.1214/12-BA703",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "03",
number = "1",
pages = "73--108",
publisher = "International Society for Bayesian Analysis",
title = "Scalable Variational Inference for Bayesian Variable Selection in Regression, and Its Accuracy in Genetic Association Studies",
url = "https://doi.org/10.1214/12-BA703",
volume = "7",
year = "2012"
}


@article{Sanguinetti2006,
    author = {Sanguinetti, Guido and Lawrence, Neil D. and Rattray, Magnus},
    title = "{Probabilistic inference of transcription factor concentrations and gene-specific regulatory activities}",
    journal = {Bioinformatics},
    volume = {22},
    number = {22},
    pages = {2775-2781},
    year = {2006},
    month = {09},
    abstract = "{Motivation: Quantitative estimation of the regulatory relationship between transcription factors and genes is a fundamental stepping stone when trying to develop models of cellular processes. Recent experimental high-throughput techniques, such as Chromatin Immunoprecipitation (ChIP) provide important information about the architecture of the regulatory networks in the cell. However, it is very difficult to measure the concentration levels of transcription factor proteins and determine their regulatory effect on gene transcription. It is therefore an important computational challenge to infer these quantities using gene expression data and network architecture data.Results: We develop a probabilistic state space model that allows genome-wide inference of both transcription factor protein concentrations and their effect on the transcription rates of each target gene from microarray data. We use variational inference techniques to learn the model parameters and perform posterior inference of protein concentrations and regulatory strengths. The probabilistic nature of the model also means that we can associate credibility intervals to our estimates, as well as providing a tool to detect which binding events lead to significant regulation. We demonstrate our model on artificial data and on two yeast datasets in which the network structure has previously been obtained using ChIP data. Predictions from our model are consistent with the underlying biology and offer novel quantitative insights into the regulatory structure of the yeast cell.Availability: MATLAB code is available from Contact:guido@dcs.shef.ac.ukSupplementary information: Supplementary Data are available at Bioinformatics online}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btl473},
    url = {https://dx.doi.org/10.1093/bioinformatics/btl473},
    eprint = {http://oup.prod.sis.lan/bioinformatics/article-pdf/22/22/2775/16851948/btl473.pdf},
}




@article {Raj2014,
	author = {Raj, Anil and Stephens, Matthew and Pritchard, Jonathan K.},
	title = {fastSTRUCTURE: Variational Inference of Population Structure in Large SNP Data Sets},
	volume = {197},
	number = {2},
	pages = {573--589},
	year = {2014},
	doi = {10.1534/genetics.114.164350},
	publisher = {Genetics},
	abstract = {Tools for estimating population structure from genetic data are now used in a wide variety of applications in population genetics. However, inferring population structure in large modern data sets imposes severe computational challenges. Here, we develop efficient algorithms for approximate inference of the model underlying the STRUCTURE program using a variational Bayesian framework. Variational methods pose the problem of computing relevant posterior distributions as an optimization problem, allowing us to build on recent advances in optimization theory to develop fast inference tools. In addition, we propose useful heuristic scores to identify the number of populations represented in a data set and a new hierarchical prior to detect weak population structure in the data. We test the variational algorithms on simulated data and illustrate using genotype data from the CEPH{\textendash}Human Genome Diversity Panel. The variational algorithms are almost two orders of magnitude faster than STRUCTURE and achieve accuracies comparable to those of ADMIXTURE. Furthermore, our results show that the heuristic scores for choosing model complexity provide a reasonable range of values for the number of populations represented in the data, with minimal bias toward detecting structure when it is very weak. Our algorithm, fastSTRUCTURE, is freely available online at http://pritchardlab.stanford.edu/structure.html.},
	issn = {0016-6731},
	URL = {http://www.genetics.org/content/197/2/573},
	eprint = {http://www.genetics.org/content/197/2/573.full.pdf},
	journal = {Genetics}
}


@ARTICLE{Likas2004, 
author={A. C. {Likas} and N. P. {Galatsanos}}, 
journal={IEEE Transactions on Signal Processing}, 
title={A variational approach for Bayesian blind image deconvolution}, 
year={2004}, 
volume={52}, 
number={8}, 
pages={2222-2233}, 
keywords={variational techniques;Bayes methods;deconvolution;image restoration;learning (artificial intelligence);optimisation;variational approach;Bayesian blind image deconvolution;machine learning;expectation maximization algorithm;parameter estimation;image restoration;graphical models;iterative algorithms;point spread function;Bayesian methods;Deconvolution;Machine learning algorithms;Atmospheric measurements;Extraterrestrial measurements;Maximum likelihood estimation;Machine learning;Approximation algorithms;Costs;Parameter estimation}, 
doi={10.1109/TSP.2004.831119}, 
ISSN={1053-587X}, 
month={Aug},}


@article{Blei2003,
 author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
 title = {Latent Dirichlet Allocation},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {993--1022},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=944919.944937},
 acmid = {944937},
 publisher = {JMLR.org},
} 
[download]


@article{Nakajima2007,
	Author = {Nakajima, Shinichi and Watanabe, Sumio},
	Journal = {Neural Computation},
	Month = {2019/03/14},
	Number = {4},
	Pages = {1112--1153},
	Title = {Variational Bayes Solution of Linear Neural Networks and Its Generalization Performance},
	Volume = {19},
	Year = {2007}}


@inproceedings{Barber1998,
  title={Tractable Variational Structures for Approximating Graphical Models},
  author={David Barber and Wim Wiegerinck},
  booktitle={NIPS},
  year={1998}
}

@incollection{Lawrence1996,
title = {Exploiting Tractable Substructures in Intractable Networks},
author = {Lawrence K. Saul and Michael I. Jordan},
booktitle = {Advances in Neural Information Processing Systems 8},
editor = {D. S. Touretzky and M. C. Mozer and M. E. Hasselmo},
pages = {486--492},
year = {1996},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1155-exploiting-tractable-substructures-in-intractable-networks.pdf}
}



@ARTICLE{Wang2012,
       author = {{Wang}, Chong and {Blei}, David M.},
        title = "{Variational Inference in Nonconjugate Models}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning},
         year = "2012",
        month = "Sep",
          eid = {arXiv:1209.4360},
        pages = {arXiv:1209.4360},
archivePrefix = {arXiv},
       eprint = {1209.4360},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2012arXiv1209.4360W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{Khan2017,
       author = {{Emtiyaz Khan}, Mohammad and {Lin}, Wu},
        title = "{Conjugate-Computation Variational Inference : Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2017",
        month = "Mar",
          eid = {arXiv:1703.04265},
        pages = {arXiv:1703.04265},
archivePrefix = {arXiv},
       eprint = {1703.04265},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv170304265E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Blei2006,
author = "Blei, David M. and Jordan, Michael I.",
doi = "10.1214/06-BA104",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "03",
number = "1",
pages = "121--143",
publisher = "International Society for Bayesian Analysis",
title = "Variational inference for Dirichlet process mixtures",
url = "https://doi.org/10.1214/06-BA104",
volume = "1",
year = "2006"
}

@ARTICLE{Komili2008,
  title     = "Coupling and coordination in gene expression processes: a
               systems biology view",
  author    = "Komili, Suzanne and Silver, Pamela A",
  abstract  = "Genome-scale analyses have allowed us to progress beyond
               studying gene expression at the level of individual components
               of a given process by providing global information about
               functional connections between genes, mRNAs and their regulatory
               proteins. Such analyses have greatly increased our understanding
               of the interplay between different events in gene regulation and
               have highlighted previously unappreciated functional
               connections, including coupling between nuclear and cytoplasmic
               processes. Genome-wide approaches have also revealed extensive
               coordination within regulatory levels, such as the organization
               of transcription factors into regulatory motifs. Overall, these
               studies enhance our understanding of how the many components of
               the eukaryotic cell function as a system to allow both
               coordination and versatility in gene expression.",
  journal   = "Nat. Rev. Genet.",
  publisher = "Nature Publishing Group",
  volume    =  9,
  pages     = "38",
  month     =  jan,
  year      =  2008
}

@ARTICLE{Leek2007,
  title     = "Capturing Heterogeneity in Gene Expression Studies by Surrogate
               Variable Analysis",
  author    = "Leek, Jeffrey T and Storey, John D",
  abstract  = "Author SummaryIn scientific and medical studies, great care must
               be taken when collecting data to understand the relationship
               between two variables, such as a drug and its effect on a
               disease. In any given study there will be many other variables
               at play, such as the effects of age and sex on the disease. We
               show that in studies where the expression levels of thousands of
               genes are measured at once, these issues become surprisingly
               critical. Due to the complexity of our genomes, environment, and
               demographic features, there are many sources of variation when
               analyzing gene expression levels. In any given study, it is
               impossible to measure every single variable that may be
               influencing how our genes are expressed. Despite this, we show
               that by considering all expression levels simultaneously, one
               can actually recover the effects of these important missed
               variables and essentially produce an analysis as if all relevant
               variables were included. As opposed to traditional studies, the
               massive amount of data available in this setting is what makes
               the method, called surrogate variable analysis, possible. We
               hypothesize that surrogate variable analysis will be useful in
               many large-scale gene expression studies.",
  journal   = "PLoS Genet.",
  publisher = "Public Library of Science",
  volume    =  3,
  number    =  9,
  pages     = "e161",
  month     =  sep,
  year      =  2007
}

@ARTICLE{Stegle2012,
  title    = "Using probabilistic estimation of expression residuals ({PEER})
              to obtain increased power and interpretability of gene expression
              analyses",
  author   = "Stegle, Oliver and Parts, Leopold and Piipari, Matias and Winn,
              John and Durbin, Richard",
  abstract = "We present PEER (probabilistic estimation of expression
              residuals), a software package implementing statistical models
              that improve the sensitivity and interpretability of genetic
              associations in population-scale expression data. This approach
              builds on factor analysis methods that infer broad variance
              components in the measurements. PEER takes as input transcript
              profiles and covariates from a set of individuals, and then
              outputs hidden factors that explain much of the expression
              variability. Optionally, these factors can be interpreted as
              pathway or transcription factor activations by providing prior
              information about which genes are involved in the pathway or
              targeted by the factor. The inferred factors are used in genetic
              association analyses. First, they are treated as additional
              covariates, and are included in the model to increase detection
              power for mapping expression traits. Second, they are analyzed as
              phenotypes themselves to understand the causes of global
              expression variability. PEER extends previous related surrogate
              variable models and can be implemented within hours on a desktop
              computer.",
  journal  = "Nat. Protoc.",
  volume   =  7,
  number   =  3,
  pages    = "500--507",
  month    =  feb,
  year     =  2012,
  language = "en"
}




@article{Pournara2007,
	Author = {Pournara, Iosifina and Wernisch, Lorenz},
	Journal = {BMC Bioinformatics},
	Number = {1},
	Pages = {61},
	Title = {Factor analysis for gene regulatory networks and transcription factor activity profiles},
	Volume = {8},
	Year = {2007}}


@article{Dai2017,
    author = {Lawrence, Neil D and Dai, Zhenwen and Rattray, Magnus and Iqbal, Mudassar},
    title = "{Efficient inference for sparse latent variable models of transcriptional regulation}",
    journal = {Bioinformatics},
    volume = {33},
    number = {23},
    pages = {3776-3783},
    year = {2017},
    month = {08},
    abstract = "{Regulation of gene expression in prokaryotes involves complex co-regulatory mechanisms involving large numbers of transcriptional regulatory proteins and their target genes. Uncovering these genome-scale interactions constitutes a major bottleneck in systems biology. Sparse latent factor models, assuming activity of transcription factors (TFs) as unobserved, provide a biologically interpretable modelling framework, integrating gene expression and genome-wide binding data, but at the same time pose a hard computational inference problem. Existing probabilistic inference methods for such models rely on subjective filtering and suffer from scalability issues, thus are not well-suited for realistic genome-scale applications.We present a fast Bayesian sparse factor model, which takes input gene expression and binding sites data, either from ChIP-seq experiments or motif predictions, and outputs active TF-gene links as well as latent TF activities. Our method employs an efficient variational Bayes scheme for model inference enabling its application to large datasets which was not feasible with existing MCMC-based inference methods for such models. We validate our method on synthetic data against a similar model in the literature, employing MCMC for inference, and obtain comparable results with a small fraction of the computational time. We also apply our method to large-scale data from Mycobacterium tuberculosis involving ChIP-seq data on 113 TFs and matched gene expression data for 3863 putative target genes. We evaluate our predictions using an independent transcriptomics experiment involving over-expression of TFs.An easy-to-use Jupyter notebook demo of our method with data is available at https://github.com/zhenwendai/SITAR.Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btx508},
    url = {https://dx.doi.org/10.1093/bioinformatics/btx508},
    eprint = {http://oup.prod.sis.lan/bioinformatics/article-pdf/33/23/3776/25168082/btx508.pdf},
}




@article{Genevieve2018,
	Author = {Stein-O'Brien, Genevieve L. and Arora, Raman and Culhane, Aedin C. and Favorov, Alexander V. and Garmire, Lana X. and Greene, Casey S. and Goff, Loyal A. and Li, Yifeng and Ngom, Aloune and Ochs, Michael F. and Xu, Yanxun and Fertig, Elana J.},
	Journal = {Trends in Genetics},
	Number = {10},
	Pages = {790--805},
	Title = {Enter the Matrix: Factorization Uncovers Knowledge from Omics},
	Volume = {34},
	Year = {2018}}


@ARTICLE{Lawrence2005,
  title    = "Probabilistic Non-linear Principal Component Analysis with
              Gaussian Process Latent Variable Models",
  author   = "Lawrence, Neil",
  journal  = "J. Mach. Learn. Res.",
  volume   =  6,
  number   = "Nov",
  pages    = "1783--1816",
  year     =  2005
}


@article{Rubin1982,
	Author = {Rubin, Donald B. and Thayer, Dorothy T.},
	Journal = {Psychometrika},
	Number = {1},
	Pages = {69--76},
	Title = {EM algorithms for ML factor analysis},
	Volume = {47},
	Year = {1982}}

@book{basilevsky2009statistical,
	title={Statistical factor analysis and related methods: theory and applications},
	author={Basilevsky, Alexander T},
	volume={418},
	year={2009},
	publisher={John Wiley \& Sons}
}




@article{Leppaaho2017,
  title={GFA: exploratory analysis of multiple data sources with group factor analysis},
  author={Lepp{\"a}aho, Eemeli and Kaski, Samuel},
  journal={Journal of Machine Learning Research},
  volume={18},
  pages={1--5},
  year={2017}
}



@inproceedings{Virtanen2012,
	title={Bayesian group factor analysis},
	author={Virtanen, Seppo and Klami, Arto and Khan, Suleiman and Kaski, Samuel},
	booktitle={Artificial Intelligence and Statistics},
	pages={1269--1277},
	year={2012}
}

@article{Klami2015,
	title={Group factor analysis},
	author={Klami, Arto and Virtanen, Seppo and Lepp{\"a}aho, Eemeli and Kaski, Samuel},
	journal={IEEE transactions on neural networks and learning systems},
	volume={26},
	number={9},
	pages={2136--2147},
	year={2015},
	publisher={IEEE}
}

@article{Bunte2016,
	title={Sparse group factor analysis for biclustering of multiple data sources},
	author={Bunte, Kerstin and Lepp{\"a}aho, Eemeli and Saarinen, Inka and Kaski, Samuel},
	journal={Bioinformatics},
	volume={32},
	number={16},
	pages={2457--2463},
	year={2016},
	publisher={Oxford Univ Press}
}


@article{Khan2014,
	title={Identification of structural features in chemicals associated with cancer drug response: a systematic data-driven analysis},
	author={Khan, Suleiman A and Virtanen, Seppo and Kallioniemi, Olli P and Wennerberg, Krister and Poso, Antti and Kaski, Samuel},
	journal={Bioinformatics},
	volume={30},
	number={17},
	pages={i497--i504},
	year={2014},
	publisher={Oxford Univ Press}
}

@article{Zhao2016,
	title={Bayesian group factor analysis with structured sparsity},
	author={Zhao, Shiwen and Gao, Chuan and Mukherjee, Sayan and Engelhardt, Barbara E},
	journal={Journal of Machine Learning Research},
	volume={17},
	number={196},
	pages={1--47},
	year={2016}
}



@article{remes2015classification,
	title={Classification of weak multi-view signals by sharing factors in a mixture of Bayesian group factor analyzers},
	author={Remes, Sami and Mononen, Tommi and Kaski, Samuel},
	journal={arXiv preprint arXiv:1512.05610},
	year={2015}
}


@article{Klami2008,
	Author = {Klami, Arto and Kaski, Samuel},
	Journal = {Neurocomputing},
	Number = {1},
	Pages = {39--46},
	Title = {Probabilistic approach to detecting dependencies between data sets},
	Volume = {72},
	Year = {2008}
}



@article{Klami2013,
 author = {Klami, Arto and Virtanen, Seppo and Kaski, Samuel},
 title = {Bayesian Canonical Correlation Analysis},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2013},
 volume = {14},
 number = {1},
 month = apr,
 year = {2013},
 issn = {1532-4435},
 pages = {965--1003},
 numpages = {39},
 url = {http://dl.acm.org/citation.cfm?id=2567709.2502612},
 acmid = {2502612},
 publisher = {JMLR.org}
} 	

@article{Li2016,
    author = {Li, Yifeng and Wu, Fang-Xiang and Ngom, Alioune},
    title = "{A review on machine learning principles for multi-view biological data integration}",
    journal = {Briefings in Bioinformatics},
    volume = {19},
    number = {2},
    pages = {325-340},
    year = {2016},
    month = {12},
    abstract = "{Driven by high-throughput sequencing techniques, modern genomic and clinical studies are in a strong need of integrative machine learning models for better use of vast volumes of heterogeneous information in the deep understanding of biological systems and the development of predictive models. How data from multiple sources (called multi-view data) are incorporated in a learning system is a key step for successful analysis. In this article, we provide a comprehensive review on omics and clinical data integration techniques, from a machine learning perspective, for various analyses such as prediction, clustering, dimension reduction and association. We shall show that Bayesian models are able to use prior information and model measurements with various distributions; tree-based methods can either build a tree with all features or collectively make a final decision based on trees learned from each view; kernel methods fuse the similarity matrices learned from individual views together for a final similarity matrix or learning model; network-based fusion methods are capable of inferring direct and indirect associations in a heterogeneous network; matrix factorization models have potential to learn interactions among features from different views; and a range of deep neural networks can be integrated in multi-modal learning for capturing the complex mechanism of biological systems.}",
    issn = {1477-4054},
    doi = {10.1093/bib/bbw113},
    url = {https://dx.doi.org/10.1093/bib/bbw113},
    eprint = {http://oup.prod.sis.lan/bib/article-pdf/19/2/325/25524236/bbw113.pdf},
}




@ARTICLE{Xu2013,
       author = {{Xu}, Chang and {Tao}, Dacheng and {Xu}, Chao},
        title = "{A Survey on Multi-view Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2013",
        month = "Apr",
          eid = {arXiv:1304.5634},
        pages = {arXiv:1304.5634},
archivePrefix = {arXiv},
       eprint = {1304.5634},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2013arXiv1304.5634X},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@book{Hardle2007,
	author 	     = {W Hardle and L Simar},
	title        = {Applied Multivariate Statistical Analysis},
	publisher    = {Springer},
	year         = {2007},
	isbn 		 = {978-3-540-72243-4},
	pages		 = {321-30}
}

@ARTICLE{Hotelling1933,
  title    = "Analysis of a complex of statistical variables into principal components",
  author   = "Hotelling, H.",
  journal  = "Journal of Educational Psychology",
  volume   =  24,
  number   =  6,
  pages    = "417-441",
  year     =  1933
}

@article{Hotteling1936,
    author = {Hotelling, Harold},
    title = "{Relations between two sets of variates}",
    journal = {Biometrika},
    volume = {28},
    number = {3-4},
    pages = {321-377},
    year = {1936},
    month = {12},
    issn = {0006-3444},
    doi = {10.1093/biomet/28.3-4.321},
    url = {https://dx.doi.org/10.1093/biomet/28.3-4.321},
    eprint = {http://oup.prod.sis.lan/biomet/article-pdf/28/3-4/321/586830/28-3-4-321.pdf},
}




@article {McCabe2018,
	author = {McCabe, Sean D. and Lin, Dan-Yu and Love, Michael I},
	title = {MOVIE: Multi-Omics VIsualization of Estimated contributions},
	elocation-id = {379115},
	year = {2018},
	doi = {10.1101/379115},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The growth of multi-omics datasets has given rise to many methods for identifying sources of common variation across data types. The unsupervised nature of these methods makes it difficult to evaluate their performance. We present MOVIE, Multi-Omics VIsualization of Estimated contributions, as a framework for evaluating the degree of overfitting and the stability of unsupervised multi-omics methods. MOVIE plots the contributions of one data type against another to produce contribution plots, where contributions are calculated for each subject and each data type from the results of each multi-omics method. The usefulness of MOVIE is demonstrated by applying existing multi-omics methods to permuted null data and breast cancer data from The Cancer Genome Atlas. Contribution plots indicated that principal components-based Canonical Correlation Analysis overt null data, while Sparse multiple Canonical Correlation Analysis and Multi-Omics Factor Analysis provided stable results with high specificity for both the real and permuted null datasets.},
	URL = {https://www.biorxiv.org/content/early/2018/07/29/379115},
	eprint = {https://www.biorxiv.org/content/early/2018/07/29/379115.full.pdf},
	journal = {bioRxiv}
}


@article{Guo2016,
 author = {Guo, Yiwen and Ding, Xiaoqing and Liu, Changsong and Xue, Jing-Hao},
 title = {Sufficient Canonical Correlation Analysis},
 journal = {Trans. Img. Proc.},
 issue_date = {June 2016},
 volume = {25},
 number = {6},
 month = jun,
 year = {2016},
 issn = {1057-7149},
 pages = {2610--2619},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/TIP.2016.2551374},
 doi = {10.1109/TIP.2016.2551374},
 acmid = {2930053},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@TECHREPORT{Bach2005,
    author = {Francis R. Bach and Michael I. Jordan},
    title = {A probabilistic interpretation of canonical correlation analysis},
    institution = {},
    year = {2005}
}

@article{Wang2007,
	author		 = {C Wang},
	title		 = {Variational Bayesian Approach to Canonical Correlation Analysis},
	journaltitle = {IEEE Trans Neural Netw},
	number 		 = {18},
	volume		 = {3},
	year         = {2007}
}

@Misc{Dietz2010-technical-report-graphs,
    title    = "Directed Factor Graph Notation for Generative Models",
    author   = "Dietz, Laura",
    journal  = "Technical Report",
    year     =  2010
}

@ARTICLE{Meng2016,
  title     = "Dimension reduction techniques for the integrative analysis of
               multi-omics data",
  author    = "Meng, Chen and Zeleznik, Oana A and Thallinger, Gerhard G and
               Kuster, Bernhard and Gholami, Amin M and Culhane, Aed{\'\i}n C",
  abstract  = "State-of-the-art next-generation sequencing, transcriptomics,
               proteomics and other high-throughput `omics' technologies enable
               the efficient generation of large experimental data sets. These
               data may yield unprecedented knowledge about molecular pathways
               in cells and their role in disease. Dimension reduction
               approaches have been widely used in exploratory analysis of
               single omics data sets. This review will focus on dimension
               reduction approaches for simultaneous exploratory analyses of
               multiple data sets. These methods extract the linear
               relationships that best explain the correlated structure across
               data sets, the variability both within and between variables (or
               observations) and may highlight data issues such as batch
               effects or outliers. We explore dimension reduction techniques
               as one of the emerging approaches for data integration, and how
               these can be applied to increase our understanding of biological
               systems in normal physiological function and disease.",
  journal   = "Brief. Bioinform.",
  publisher = "Oxford University Press",
  volume    =  17,
  number    =  4,
  pages     = "628--641",
  month     =  jul,
  year      =  2016
}


@article{Zhang2018_NN,
  title={Opening the black box of neural networks: methods for interpreting neural network models in clinical applications.},
  author={Zhongheng Zhang and Marcus W. Beck and David A. Winkler and Bin Huang and Wilbert Sibanda and Hemant Goyal},
  journal={Annals of translational medicine},
  year={2018},
  volume={6 11},
  pages={
          216
        }
}

@ARTICLE{Ringner2008,
  title     = "What is principal component analysis?",
  author    = "Ringn{\'e}r, Markus",
  abstract  = "Principal component analysis is often incorporated into
               genome-wide expression studies, but what is it and how can it be
               used to explore high-dimensional data?",
  journal   = "Nat. Biotechnol.",
  publisher = "Nature Publishing Group",
  volume    =  26,
  pages     = "303",
  month     =  mar,
  year      =  2008
}

@article{Rattray2009,
	doi = {10.1088/1742-6596/197/1/012002},
	url = {https://doi.org/10.1088%2F1742-6596%2F197%2F1%2F012002},
	year = 2009,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {197},
	pages = {012002},
	author = {Magnus Rattray and Oliver Stegle and Kevin Sharp and John Winn},
	title = {Inference algorithms and learning theory for Bayesian sparse factor analysis},
	journal = {Journal of Physics: Conference Series},
	abstract = {Bayesian sparse factor analysis has many applications; for example, it has been applied to the problem of inferring a sparse regulatory network from gene expression data. We describe a number of inference algorithms for Bayesian sparse factor analysis using a slab and spike mixture prior. These include well-established Markov chain Monte Carlo (MCMC) and variational Bayes (VB) algorithms as well as a novel hybrid of VB and Expectation Propagation (EP). For the case of a single latent factor we derive a theory for learning performance using the replica method. We compare the MCMC and VB/EP algorithm results with simulated data to the theoretical prediction. The results for MCMC agree closely with the theory as expected. Results for VB/EP are slightly sub-optimal but show that the new algorithm is effective for sparse inference. In large-scale problems MCMC is infeasible due to computational limitations and the VB/EP algorithm then provides a very useful computationally efficient alternative.}
}


@article{Li2017,
	Author = {Li, Ziyi and Safo, Sandra E. and Long, Qi},
	Journal = {BMC Bioinformatics},
	Number = {1},
	Pages = {332},
	Title = {Incorporating biological information in sparse principal component analysis with application to genomic data},
	Volume = {18},
	Year = {2017}}

@article{Ilin2010,
 author = {Ilin, Alexander and Raiko, Tapani},
 title = {Practical Approaches to Principal Component Analysis in the Presence of Missing Values},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 month = aug,
 year = {2010},
 issn = {1532-4435},
 pages = {1957--2000},
 numpages = {44},
 url = {http://dl.acm.org/citation.cfm?id=1756006.1859917},
 acmid = {1859917},
 publisher = {JMLR.org}
} 


@ARTICLE{Tipping1999,
  title     = {Probabilistic Principal Component Analysis},
  author    = {Tipping, ME and CM Bishop},
  publisher = {Springer-Verlag},
  year      =  {1999},
  journal={Journal of the Royal Statistical Society},
  volume={61(3)},
  pages={611â€“22}
}

@Misc{Neal1995,
  title = {Bayesian learning for neural networks},
  author = {Radford M. Neal},
  year=1995,
  institution={University of Toronto}
}

@inproceedings{Bishop1999a,
 author = {Bishop, Christopher M.},
 title = {Bayesian PCA},
 booktitle = {Proceedings of the 1998 Conference on Advances in Neural Information Processing Systems II},
 year = {1999},
 isbn = {0-262-11245-0},
 pages = {382--388},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=340534.340674},
 acmid = {340674},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@Inproceedings{Bishop1999b,
author = {Bishop, Christopher},
title = {Variational Principal Components},
booktitle = {Proceedings Ninth International Conference on Artificial Neural Networks, ICANN'99},
year = {1999},
month = {January},
abstract = {One of the central issues in the use of principal component analysis (PCA) for data modelling is that of choosing the appropriate number of retained components. This problem was recently addressed through the formulation of a Bayesian treatment of PCA (Bishop, 1998) in terms of a probabilistic latent variable model. A central feature of this approach is that the effective dimensionality of the latent space (equivalent to the number of retained principal components) is determined automatically as part of the Bayesian inference procedure. In common with most non-trivial Bayesian models, however, the required marginalizations are analytically intractable, and so an approximation scheme based on a local Gaussian representation of the posterior distribution was employed. In this paper we develop an alternative, variational formulation of Bayesian PCA, based on a factorial representation of the posterior distribution. This approach is computationally efficient, and unlike other approximation schemes, it maximizes a rigourous lower bound on the marginal log probability of the observed data.
},
publisher = {IEE},
url = {https://www.microsoft.com/en-us/research/publication/variational-principal-components/},
pages = {509-514},
volume = {1},
edition = {Proceedings Ninth International Conference on Artificial Neural Networks, ICANN'99},
}

@ARTICLE{Huang2017,
  title   = "More Is Better: Recent Progress in {Multi-Omics} Data Integration
             Methods",
  author  = "Huang, Sijia and Chaudhary, Kumardeep and Garmire, Lana X",
  journal = "Front. Genet.",
  volume  =  8,
  pages   = "1005",
  month   =  jun,
  year    =  2017
}


@article{Hasin2017,
	Author = {Hasin, Yehudit and Seldin, Marcus and Lusis, Aldons},
	Journal = {Genome Biology},
	Number = {1},
	Pages = {83},
	Title = {Multi-omics approaches to disease},
	Volume = {18},
	Year = {2017}}



@article{Zeng2018,
	Author = {Zeng, Irene Sui Lan and Lumley, Thomas},
	Journal = {Bioinformatics and Biology Insights},
	Month = {2019/03/16},
	Pages = {1177932218759292},
	Title = {Review of Statistical Learning Methods in Integrated Omics Studies (An Integrated Information Science)},
	Volume = {12},
	Year = {2018}}


@article{Pilling2018,
author = {Pilling, Mark},
title = {Handbook of Applied Modelling: Non-Gaussian and Correlated Data},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
volume = {181},
number = {4},
pages = {1264-1265},
doi = {10.1111/rssa.12402},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12402},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12402},
year = {2018}
}
