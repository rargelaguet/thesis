\subsection{Variational updates}
In this section, we give the explicit update equations for every hidden variable that is applied at each iteration of the (stochastic) variational bayes algorithm.\\
Additionally, we write down the analytical form of the ELBO. Although computing the ELBO is not necessary in order to estimate the posterior distribution of the parameters, it is used to monitor the convergence of the algorithm.

\subsection{Variational Updates}

\subsubsection{Non-sparse factors}
For every group $g$, sample $n$ and factor $k$: \\

Prior distribution $p(z_{nk}^g)$:\\
\[
	p(z_{nk}^g), = \mathcal{N} (z_{nk}^g |0,1)
\]
Variational distribution $q(z^g_{nk})$:\\
\begin{equation}
   q(z^g_{nk}) = \mathcal{N} ( z_{nk}^g | \mu_{z_{nk}^g}, \sigma_{z_{nk}^g} )
\end{equation}
where
\begin{equation} \begin{aligned}
	\sigma_{z_{nk}^{g}}^2 &= \left( \sum_{m=1}^{M} \sum_{d=1}^{D_m} \la\tau_{dg}^m \ra \la (w_{dk}^m)^2 \ra + 1 \right)^{-1}\\
	\mu_{z_{nk}^g} &= \sigma_{z_{nk}^g}^2 \sum_{m=1}^{M} \sum_{d=1}^{D_m} \la\tau_{dg}^m\ra \la w_{dk}^m \ra \left(\sum_{g=1}^{G} y_{nd}^{gm} - \sum_{j \neq k} \la w_{dj}^m \ra \la z_{nj}^g \ra \right)
\end{aligned} \end{equation}


\subsubsection{Sparse factors}
For every group $g$, sample $n$ and factor $k$: \\

Prior distribution $p(\hat{z}_{nk}^g,s_{nk}^g)$:\\

\begin{align}
	p(\hat{z}_{nk}^g,s_{nk}^g) &= \mathcal{N} (\hat{z}_{nk}^g \,|\, 0, 1/\alpha_k^g)\, \text{Ber}(s_{nk}^g \,|\,\theta_k^g) \\
	p(\theta_k^g) &= \Bdist{\theta_k^g}{a_0^\theta,b_0^\theta}\\
	p(\alpha_k^g) &= \Gdist{\alpha_k^g}{a_0^\alpha, b_0^\alpha},
\end{align}

Variational distribution:\\
TO-DO

\subsubsection{group-wise ARD precision for the factors}
\subsubsection{Spike-and-slab sparsity for the factors}

    % \paragraph{Sample-wise}

    %     Variational distribution:\\

    %     \begin{equation}
    %         q(\alpha^g_{k}) = \Gamma \left( \alpha_k^g \middle| \hat{a}_{g,k}^{\alpha}, \hat{b}_{g,k}^{\alpha} \right)
    %     \end{equation}

    %     where\\

    %     \begin{equation}
    %       \begin{aligned}
    %           \hat{a}_{g,k}^\alpha &= a_0^\alpha + \frac{N_g}{2}\\
    %           \hat{b}_{g,k}^\alpha &= b_0^\alpha +\frac{ \sum_{n=1}^{N_g} \la (z_{nk}^g)^2 \ra }{2}
    %       \end{aligned}
    %     \end{equation}


% START COPIED
% Prior distribution $p(\hat{w}_{dk}^m,s_{dk}^m)$:\\
% \begin{align}
% 	p(\hat{w}_{dk}^m,s_{dk}^m) &= \mathcal{N} (\hat{w}_{dk}^m \,|\, 0, 1/\alpha_k^m)\, \text{Ber}(s_{dk}^m \,|\,\theta_k^m)
% \end{align}

% Variational distribution $q(\hat{w}_{dk}^m,s_{dk}^m)$:\\

% Update for $q(s_{dk}^m)$:\\

% \begin{equation}
% 	q(s^m_{dk}) = \mathrm{Ber}(s^m_{dk}|\gamma^m_{dk})
% \end{equation}
% with
% \begin{equation} \begin{aligned}
% 	&\gamma^m_{dk} = \frac{1}{1+\exp(-\lambda_{dk}^m)}\\
% 	& \lambda_{dk}^m = \la \ln\frac{\theta}{1-\theta} \ra + 0.5\ln\frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} - 0.5\ln\left( \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} \right) \\
% 	&+ \frac{\la\tau_d^m\ra}{2} \frac{ \left( \sum_{g=1}^G\sum_{n=1}^{N_g} y_{nd}^m \la z_{nk} \ra - \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m\ra \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk} \ra \la z_{nj} \ra \right)^2} {\sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
% \end{aligned} \end{equation}

% Update for $q(\hat{w}_{dk}^m)$:\\

% \begin{equation} \begin{aligned}
%       q(\hat{w}_{dk}^m|s_{dk}^m=0) &= \mathcal{N} \left(\hat{w}_{dk}^m \middle| 0, 1/\alpha_k^m \right) \\
%       q(\hat{w}_{dk}^m|s_{dk}^m=1) &= \mathcal{N} \left( \hat{w}_{dk}^m \middle| \mu_{w_{dk}^m}, \sigma_{w_{dk}^m}^2\right)
%   \end{aligned} \end{equation}
% with
% \begin{equation} \begin{aligned}
%   	\mu_{w_{dk}^m} &= \frac{ \sum_{g=1}^G\sum_{n=1}^{N_g} y_{nd}^m \la z_{nk} \ra - \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m \ra \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk} \ra \la z_{nj} \ra } { \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }\\
%   	\sigma_{w_{dk}^m} &= \frac{ \la\tau_d^m\ra^{-1} } { \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
% \end{aligned} \end{equation}
% where $N$ is the total number of samples (across all groups).

% END COPIED


\subsubsection{Sparse weights}
For every view $m$, feature $d$ and factor $k$: \\

Prior distribution $p(\hat{w}_{dk}^m,s_{dk}^m)$:\\
\begin{align}
	p(\hat{w}_{dk}^m,s_{dk}^m) &= \mathcal{N} (\hat{w}_{dk}^m \,|\, 0, 1/\alpha_k^m)\, \text{Ber}(s_{dk}^m \,|\,\theta_k^m)
\end{align}

Variational distribution $q(\hat{w}_{dk}^m,s_{dk}^m)$:\\

Update for $q(s_{dk}^m)$:\\

\begin{equation}
	q(s^m_{dk}) = \mathrm{Ber}(s^m_{dk}|\gamma^m_{dk})
\end{equation}
with
\begin{equation} \begin{aligned}
	&\gamma^m_{dk} = \frac{1}{1+\exp(-\lambda_{dk}^m)}\\
	& \lambda_{dk}^m = \la \ln\frac{\theta}{1-\theta} \ra + 0.5\ln\frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} - 0.5\ln\left( \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} \right) \\
	&+ \frac{\la\tau_d^m\ra}{2} \frac{ \left( \sum_{g=1}^G\sum_{n=1}^{N_g} y_{nd}^m \la z_{nk} \ra - \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m\ra \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk} \ra \la z_{nj} \ra \right)^2} {\sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
\end{aligned} \end{equation}

Update for $q(\hat{w}_{dk}^m)$:\\

\begin{equation} \begin{aligned}
      q(\hat{w}_{dk}^m|s_{dk}^m=0) &= \mathcal{N} \left(\hat{w}_{dk}^m \middle| 0, 1/\alpha_k^m \right) \\
      q(\hat{w}_{dk}^m|s_{dk}^m=1) &= \mathcal{N} \left( \hat{w}_{dk}^m \middle| \mu_{w_{dk}^m}, \sigma_{w_{dk}^m}^2\right)
  \end{aligned} \end{equation}
with
\begin{equation} \begin{aligned}
  	\mu_{w_{dk}^m} &= \frac{ \sum_{g=1}^G\sum_{n=1}^{N_g} y_{nd}^m \la z_{nk} \ra - \sum_{j \neq k} \la s_{dj}^m\hat{w}_{dj}^m \ra \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk} \ra \la z_{nj} \ra } { \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }\\
  	\sigma_{w_{dk}^m} &= \frac{ \la\tau_d^m\ra^{-1} } { \sum_{g=1}^G\sum_{n=1}^{N_g} \la z_{nk}^2 \ra + \frac{\la\alpha_k^m\ra}{\la\tau_d^m\ra} }
\end{aligned} \end{equation}
where $N$ is the total number of samples (across all groups).

  	% Taken together this means that we can update $q(\hat{w}_{dk}^m,s_{dk}^m)$ using:
  	% \begin{equation*}
  	% q(\hat{w}_{dk}^m|s_{dk}^m) q(s_{dk}^m) = \Ndist{ \hat{w}_{dk}^m } { s_{dk}^m \mu_{w_{dk}^m}, s_{dk}^m\sigma_{w_{dk}^m}^2 + (1-s_{dk}^m)/\alpha_k^m}    (\new{\gamma_{dk}^m})^{s_{dk}^m} (1-\new{\gamma_{dk}^m})^{1-s_{dk}}
  	% \end{equation*}


\subsubsection{ARD precision for the loadings}
For every view $m$ and factor $k$: \\

Prior distribution $p(\alpha_k^m)$:
\[
	p(\alpha_k^m) = \Gdist{\alpha_k^m}{a_0^\alpha, b_0^\alpha},
\]
Variational distribution $q(\alpha_k^m)$:
\begin{equation}
    q(\alpha^m_{k}) = \Gamma \left( \alpha_k^m \middle| \hat{a}_{mk}^{\alpha}, \hat{b}_{mk}^{\alpha} \right)
\end{equation}
where:
\begin{equation} \begin{aligned}
	\hat{a}_{mk}^\alpha &= a_0^\alpha + \frac{D_m}{2}\\
	\hat{b}_{mk}^\alpha &= b_0^\alpha +\frac{ \sum_{d=1}^{D_m} \la (\hat{w}_{dk}^m)^2 \ra }{2}
\end{aligned} \end{equation}


\subsubsection{Spike-and-slab sparsity for the loadings}
For every view $m$ and factor $k$: \\

Prior distribution:
\[
	p(\theta_k^m) = \Bdist{\theta_k^m}{a_0^\theta,b_0^\theta}
\]
Variational distribution:
\begin{equation}
	q(\theta_k^m) = \Bdist{\theta_k^m}{\hat{a}_{mk}^{\theta}, \hat{b}_{mk}^{\theta}}
\end{equation}
where
\begin{equation}
     \begin{aligned}
  	\hat{a}_{mk}^\theta &= \sum_{d=1}^{D_m} \la s^m_{dk}\ra + a_0^\theta\\
  	\hat{b}_{mk}^\theta &= b_0^\theta - \sum_{d=1}^{D_m} \la s^m_{dk}\ra + D_m
     \end{aligned}
\end{equation}


\subsubsection{Noise}
For every view $m$, group $g$ and feature $d$:\\

Prior distribution $p(\tau_{dg}^m)$:
\[
	p(\tau_{dg}^m) = \Gdist{\tau_{dg}^m}{a_0^\tau,b_0^\tau},
\]

Variational distribution $q(\tau_d^m)$:
\begin{equation}
	q(\tau_{dg}^m) = \Gdist{\tau_{dg}^m}{\hat{a}_{dg}^{m} , \hat{b}_{dg}^{m}}
\end{equation}
where:
\begin{equation} \begin{aligned}
	\hat{a}_{dg}^{m} &= a_0^{\tau} + \frac{N_g}{2}\\
	\hat{b}_{dg}^{m} &= b_0^{\tau} + \frac{1}{2} \sum_{n=1}^{N_g}  \la\left(y_{nd}^{gm} - \sum_k^{K} \hat{w}_{dk}^m s_{dk}^m z_{nk}^{g}\right)^2 \ra
\end{aligned} \end{equation}



\subsection{Evidence Lower Bound}
As shown in~\ref{section:technical:elbo_interpretation}, the ELBO can be decomposed into a sum of two terms: (1) the expected log likelihood under the current estimate of the posterior distribution of the parameters and (2) the KL divergence between the prior and the variational distributions of the parameters:\\

\begin{equation} \begin{aligned}
    \Lagr = \E_{q(\theta)} \ln P(Y|\Theta) - \KL\left(q(\Theta) \middle|\middle| p(\Theta) \right)
\end{aligned} \end{equation}


\subsection{Log likelihood term}
Assuming a gaussian likelihood:
\begin{equation} \begin{aligned}
	\E_{q(\theta)} \ln P(Y|\Theta) = & -\sum_{m=1}^M \frac{ND_m}{2} \ln(2\pi) + \sum_{g=1}^G \frac{N_g}{2} \sum_{m=1}^M \sum_{d=1}^{D_m} \la \ln(\tau_{dg}^m) \ra \\
	&-\sum_{g=1}^G \sum_{m=1}^M \sum_{d=1}^{D_m} \frac{\la \tau_{dg}^m \ra}{2} \sum_{n=1}^{N_g} \big( y_{nd}^{m,g} - \sum_{k=1}^{K}\la s_{dk}^m \hat{w}_{dk}^m \ra \la z_{nk}^g \ra \big)^2
\end{aligned} \end{equation}


\subsection{KL divergence terms}

Note that $\KL\left(q(\Theta) \middle|\middle| P(\Theta) \right) = \E_q(q(\Theta)) - \E_q(P(\Theta))$. \\
Below, we will write the analytical form for these two expectations.

\subsubsection*{Sparse loadings}

\begin{equation} \begin{aligned}
    \E_q[\ln p(\hat{W},S)] =& -\sum_{m=1}^{M}\frac{KD_m}{2}\ln(2\pi) + \sum_{m=1}^{M}\frac{D_m}{2}\sum_{k=1}^{K} \ln(\alpha_k^m) - \sum_{m=1}^{M} \frac{\alpha_k^m}{2} \sum_{d=1}^{D_m} \sum_{k=1}^{K} \la (\hat{w}_{dk}^m)^2 \ra \\
    & + \la \ln(\theta) \ra \sum_{m=1}^{M} \sum_{d=1}^{D_m} \sum_{k=1}^{K} \la s_{dk}^m \ra + \la \ln(1-\theta) \ra \sum_{m=1}^{M} \sum_{d=1}^{D_m}\sum_{k=1}^{K} (1- \la s_{dk}^m \ra)
\end{aligned} \end{equation}

\begin{equation} \begin{aligned}
	\E_q[\ln q(\hat{W}, S)] =&-\sum_{m=1}^{M}\frac{KD_m}{2}\ln(2\pi) + \frac{1}{2}\sum_{m=1}^{M}\sum_{d=1}^{D_m}\sum_{k=1}^{K}\ln(\la s_{dk}^m \ra \sigma_{w_{dk}^m}^2 + (1-\la s_{dk}^m \ra)/\alpha_k^m) \\
	&+ \sum_{m=1}^{M} \sum_{d=1}^{D_m} \sum_{k=1}^{K} (1-\la s_{dk}^m \ra) \ln(1 - \la s_{dk}^m \ra) - \la s_{dk}^m \ra \ln \la s_{dk}^m \ra
\end{aligned} \end{equation}

\subsubsection*{Sparse factors}

\begin{equation} \begin{aligned}
    \E_q[\ln p(\hat{Z},S)] =& -\sum_{g=1}^{G}\frac{N_g K}{2}\ln(2\pi) + \sum_{g=1}^{G}\frac{N_g}{2}\sum_{k=1}^{K} \ln(\alpha_k^g) - \sum_{g=1}^{G} \frac{\alpha_k^g}{2} \sum_{n=1}^{N_g} \sum_{k=1}^{K} \la (\hat{z}_{nk}^g)^2 \ra \\
    & + \la \ln(\theta) \ra \sum_{g=1}^{G} \sum_{n=1}^{N_g} \sum_{k=1}^{K} \la s_{nk}^g \ra + \la \ln(1-\theta) \ra \sum_{g=1}^{G} \sum_{n=1}^{N_g}\sum_{k=1}^{K} (1- \la s_{nk}^g \ra)
\end{aligned} \end{equation}

\begin{equation} \begin{aligned}
	\E_q[\ln q(\hat{Z}, S)] =&-\sum_{g=1}^{G}\frac{N_g K}{2}\ln(2\pi) + \frac{1}{2}\sum_{g=1}^{G}\sum_{n=1}^{N_g}\sum_{k=1}^{K}\ln(\la s_{nk}^g \ra \sigma_{z_{nk}^g}^2 + (1-\la s_{nk}^g \ra)/\alpha_k^g) \\
	&+ \sum_{g=1}^{G} \sum_{n=1}^{N_g} \sum_{k=1}^{K} (1-\la s_{nk}^g \ra) \ln(1 - \la s_{nk}^g \ra) - \la s_{nk}^g \ra \ln \la s_{nk}^g \ra
\end{aligned} \end{equation}

\subsubsection*{Non-sparse factors}
\begin{equation} \begin{aligned}
	\E_q [\ln p(Z)] &= -\frac{NK}{2}\ln(2\pi) -\frac{1}{2} \sum_{g=1}^G\sum_{n=1}^{N_g}\sum_{k=1}^{K} \la z_{nk}^2 \ra \\
	\E_q [\ln q(Z)] &= - \frac{NK}{2}(1 + \ln(2\pi)) - \frac{1}{2}\sum_{g=1}^G\sum_{n=1}^{N_g}\sum_{k=1}^{K} \ln(\sigma_{z_{nk}}^2)
\end{aligned} \end{equation}

\subsubsection*{Automatic relevance determination for the loadings}
\begin{equation} \begin{aligned}
	\E_q [\ln p(\balpha)] &= \sum_{m=1}^{M}\sum_{k=1}^{K}\Big(a_0^\alpha\ln b_0^\alpha +   (a_0^\alpha - 1) \la \ln \alpha_k \ra - b_0^\alpha \la \alpha_k \ra - \ln \Gamma(a_0^\alpha) \Big) \\
	\E_q [\ln q(\balpha)] &= \sum_{m=1}^{M}\sum_{k=1}^{K} \Big( \hat{a}_{k}^\alpha \ln \hat{b}_{k}^\alpha + (\hat{a}_{k}^\alpha - 1) \la \ln \alpha_k \ra - \hat{b}_{k}^\alpha \la \alpha_k \ra - \ln \Gamma(\hat{a}_{k}^\alpha) \Big)
\end{aligned} \end{equation}

\subsubsection*{Noise}
\begin{equation} \begin{aligned}
	\E_q [\ln p(\btau)] &= \sum_{m=1}^{M} D_m a_0^\tau \ln b_0^\tau + \sum_{g=1}^{G}\sum_{m=1}^{M}\sum_{d=1}^{Dm} (a_0^\tau - 1) \la \ln \tau_d^{mg} \ra - \sum_{g=1}^{G}\sum_{m=1}^{M}\sum_{d=1}^{Dm} b_0^\tau \la \tau_d^{mg} \ra - \sum_{m=1}^{M} D_m \ln \Gamma(a_0^\tau)\\
	\E_q [\ln q(\btau)] &= \sum_{g=1}^{G} \sum_{m=1}^{M} \sum_{d=1}^{D_m} \left( \hat{a}_{dmg}^\tau \ln \hat{b}_{dmg}^\tau + (\hat{a}_{dmg}^\tau - 1) \la \ln \tau_d^{mg} \ra - \hat{b}_{dmg}^\tau \la \tau_d^{mg} \ra - \ln \Gamma(\hat{a}_{dmg}^\tau) \right)
\end{aligned} \end{equation}

\subsubsection*{Sparsity of the loadings}
\begin{equation} \begin{aligned}
	  &\E_q\left[ \ln p(\btheta) \right] = \sum_{m=1}^M \sum_{k=1}^K\sum_{d=1}^{D_m}\left( (a_0 - 1) \times \la \ln(\pi^m_{d, k}) \ra + (b_0 -1) \la \ln(1 - \pi^m_{d, k}) \ra - \ln (\mathrm{B} (a_0, b_0))\right) \\
	  &\E_q\left[ \ln q(\btheta) \right] = \sum_{m=1}^M \sum_{k=1}^K\sum_{d=1}^{D_m}\left( (a^m_{k,d} - 1) \times \la \ln(\pi^m_{d, k}) \ra + (b^m_{k,d} -1) \la \ln(1 - \pi^m_{d, k}) \ra - \ln (\mathrm{B} (a^m_{k,d}, b^m_{k,d})) \right) \\
\end{aligned} \end{equation}

