\graphicspath{{Introduction/Figs/}}

\chapter{Overview}

\section{Profiling multi-omics at single-cell resolution}

Next-generation sequencing technologies have revolutionised the study of biological systems by enabling the genome-wide profiling of molecular layers in un unbiased manner, including the genome \cite{Fleischmann1995} the epigenome \cite{Frommer1992} and the transcriptome \cite{Lister2008,Bainbridge2006,Nagalakshmi2008,Mortazavi2008}, among others. However, bulk sequencing approaches rely on a large number of cells to report an average molecular readout, and are hence limited for the study of complex biological processes where heterogeneity is expected at single cell resolution \cite{Griffiths2018,Papalexi2017,Patel2014}. The progressive development of low-input sequencing techniques resulted in an explosion of single-cell sequencing technologies, mostly for the transcriptome. In contrast to bulk protocols, single-cell techniques provide an unprecedented opportunity to study the molecular variation associated with cellular heterogeneity, lineage diversification and cell fate commitment \cite{Kolodziejczyk2015}.

The field of single-cell sequencing has largely been driven by the quantification of the messenger RNA (mRNA). In less than a decade, the field of single-cell transcriptomics has experienced an exponential growth of scale, driven by incremental optimisations of reagent volumes and consumable costs, as well as profound changes in the nature of the technology \cite{Svensson2018}. The earliest high-throughput scRNA-seq technologies were published between 2009 and 2011, yielding a handful of cells. In 2019, there are studies that have achieved the astonishing milestone of profiling the transcriptome for more than a million cells in a single experiment \cite{Cao2019}. With the development of efficient commercial platforms, the maturation of scRNA-sequencing technologies has provided major insights on the study of lineage diversification and cell fate commitment \cite{Kolodziejczyk2015,Griffiths2018,Papalexi2017,Patel2014}. In 2020, we are at the stage of a major endeavour to generate transcriptomic atlases for different tissues, embryos and even entire adult organisms. The most ambitious of all is the Human Cell Atlas, aimed at building a reference map for all cells in the human body\cite{Aviv2017}.

While the large majority of single-cell studies are focused on capturing RNA expression information, transcriptomic readouts provide a single dimension of cellular heterogeneity and hence contain limited information to characterise the molecular determinants of phenotypic variation \cite{Ritchie2015}. Consequently, gene expression markers have been identified for a myriad of biological systems, but the role of the accompanying epigenetic changes in driving cell fate decisions remains poorly understood \cite{Griffiths2018,Kelsey2017,Bheda2014}.

To get a better insight into the epigenetics of cell fate commitment, significant effort has been placed to obtain epigenetic measurements at single-cell resolution by adapting bulk methods to low-input material. This has been particularly successful for chromatin accessibility. Due to its cost-effective strategy, single-cell ATAC-seq (scATAC-seq) has become the most popular technique to map open chromatin, and is also available in an efficient commercial platform. \cite{Cusanovich2015,Cao2018,Chen2018}.\\
Other molecular layers have also been queried, including DNA methylation\cite{Smallwood2014}, histone modifications \cite{Ku2019}, chromatin conformation \cite{Ku2019}, proteomics \cite{Specht2018} and lipidomics \cite{Thiele2019}.

Despite its success in studying molecular variation, no single "-omics" technology can capture the intricacy of complex biological mechanisms. Nonetheless, the collective information has the potential to draw a more comprehensive picture of biological processes \cite{Hasin2017,Ritchie2015}. In particular, multi-omics (or multi-modal) assays have the potential to go beyond snapshots and provide a more dynamic, perhaps even mechanistic, understanding of the connection between molecular layers. Motivated by this, multi-omic data sets are receiving increasing interest across a wide range of biological domains, including cancer biology \cite{Akavia2010,Gerstung2015}, regulatory genomics \cite{Chen2016}, microbiology \cite{Kim2016} or host-pathogen interactions \cite{Soderholm2016}. 

The profiling of multi-omic readouts at the bulk level is relatively simple, as the same tissue can be dissociated into different aliquots, where each assay can be performed independently \cite{Ritchie2015}. This strategy is also used with single-cell assays, but it has the important downside that the different molecular layers cannot be unambiguously linked, hence limiting the insights that can be inferred from the data. The ultimate goal in single-cell sequencing is to obtain multiple molecular readouts from the same cell. Matched multi-modal measurements can be obtained using a variety of strategies, some of which will be discussed in this thesis. The development of these technologies will help us understand the fundamental regulatory principles that connect the different molecular layers. In addition, integrative analyses that simultaneously pool information across multiple data modalities (-omics) and across multiple studies promise to deliver a more comprehensive insights into the complex variation that underlies cellular populations \cite{Stuart2019,Colome-Tatche2018}.

Notably, the early success and rapid development of single-cell multi-modal methods has led to their recognition as Method of the year in 2019 by the journal \textit{Nature Methods}\cite{NatMethods2020}. However, their development is still in pilot stages and there is no commercial platform available, limiting its widespread use by the community. Furthermore, common challenges in (uni-modal) single-cell assays such as low coverage and high levels of technical noise become exacerbated when doing multi-modal profiling. Quoting Cole Trapnell, one of the pioneers of single-cell data analysis: \textit{When you do a multi-omic assay, you're combining all the bad things from multiple protocols}\cite{Eisenstein2020}. Thus, one of the biggest challenge in integrative multi-modal analysis is to develop statistical frameworks that are capable of uncovering biological signal across multiple data modalities while overcoming the technical biases and missing information that are inherent to single-cell experiments. 


\section{Integrative analysis}

From the computational perspective, the rapid development of single-cell technologies is introducing unprecedented challenges for the statistical community, and novel computational methods need to be developed (or adapted) for interrogating the data generated \cite{Stegle2015}.\\
The vast majority of methods for single-cell data analysis are focused on scRNA-seq. These include normalisation\cite{Lun2016a}, feature selection\cite{Townes2019}, differential expression \cite{Kharchenko2014}, clustering\cite{Kiselev2017}, cell type recognition \cite{Abdelaal2019}, pseudotime inference \cite{Haghverdi2016}, detection of gene regulatory networks and batch correction \cite{Haghverdi2018}, among others. Analysis tools have been wrapped into popular platforms such as Seurat \cite{Butler2018}, Scater \cite{McCarthy2017} and Scanpy \cite{Wolf2018}.

Despite the explosion of statistical methods for scRNA-seq data analysis, to date very few methods have been published  with the aim to perform data integration of single-cell multi-modal assays. This is probably due to the lack of large-scale data sets to apply and benchmark methods. But also, given the high levels of missing information, the inherent amounts of technical noise and the potentially large number of cells, the integrative analysis of multi-modal measurements is one of the grand challenges in single-cell data science \cite{Lahnemann2020}.

\subsection{Defining the common coordinate framework}

The first step when performing data integration is to consider a common coordinate framework to anchor the different data modalities. This defines three broad types of strategies for single-cell data integration (\Cref{fig:overview_data_integration}): 

\begin{itemize}

	\item Cells as the common coordinate (vertical integration): when the different data modalities are derived from the same cell in \textit{matched} multi-omic assays. The main advantage is that the assignment between the molecular profiles is unambiguous. In this case, vertical integration methods are aimed at detecting axes of covariation between the different data modalities. 

	\item Genomic features as the common coordinate framework (horizontal integration): when multiple data modalities of the same type are derived from different cells. We call this \textit{non-matched} multi-omics and the main advantage is that it is significantly easier and cheaper to obtain than \textit{matched} multi-omics, and as a result most of the current data sets to date belong to this category. In this case, horizontal integration strategies generally aim at finding a common biological manifold and between the data modalities, while removing variation that is not shared between experiments. This is typically addressed as a batch correction problem.

	\item No common coordinate framework in the high-dimensional space (diagonal integration): for experimental designs where both cells and genomic features are different between experiments. This is exemplified by profiling RNA expression and chromatin accessibility from independent sets of cells. In this case, one can design a horizontal integration task by summarising chromatin accessibility measurements over genomic elements that can be directly related to mRNA expression (i.e. promoters or gene bodies). However, in the most general case, chromatin accessibility and RNA expression are not necessarily derived from the same genomic elements and the data integration task becomes significantly more challenging. Diagonal integration methods typically assume the existence of a common biological manifold in a \textit{low-dimensional} space, for example by assuming that cells are sampled from the same differentiation trajectory. In the general and multivariate case, this is arguably the most complex data integration task and very few methods have been published to date.

\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{overview_data_integration}
	\caption{Defining the data integration strategy: choosing the common coordinate framework. Schematic representation of (a) Horizontal integration, when features act as anchors (b) Vertical integration, when cells act as anchors (c) Diagonal integration, when no anchors exist. }
	\label{fig:overview_data_integration}
\end{figure}

\subsection{Defining the methodology}

Once the common coordinate framework is defined, one needs to choose the data integration strategy. These can fall into two classes: \textit{local} and \textit{global}, a notation inspired from integrative approaches that have been pursued at the bulk level\cite{Ritchie2015}. 

Local analyses refer to associations between specific features across different molecular layers, with the aim of detecting putative interactions between them. Prominent examples are associations between genetic variants and gene expression (expression quantitative trait loci, eQTL) or correlations between the epigenetic status of putative regulatory elements and gene expression of nearby genes. The restriction to a local search space is often necessary, to maintain the problem tractable. For example, cis eQTL mapping is most prevalent because by testing only proximal genetic variants for each gene, the effects of the multiple testing burden is reduced. Since such association analyses are typically performed per feature (across cells), \textit{local} analyses generally requires unambiguous matching between the modalities, as given only by matched multi-modal assays, and thus belongs to the category of vertical analysis. As for methods, most \textit{local} analyses rely on different flavours of linear regression models, with different modelling assumptions depending on the nature of the molecular readouts. This can include non-gaussian likelihoods, sparsity assumptions to prevent overfitting or probabilistic terms to account for random effects. For example, linear mixed models (LMMs) are a popular framework for performing genetic analyses \cite{Moore2019}. In a LMM, a random effect term is added to account for the population structure and relatedness between individuals, which may affect both the phenotype and the genotype, thus leading to spurious associations if not accounted for.

While useful for characterising genetic variants or identifying putative regulatory elements, local analysis have limited capacity to discover complex maps of molecular heterogeneity that result from interactions between genomic features. An alternative strategy for data integration is to exploit the full spectrum of measurements to identify cellular states defined by the coordinated action of multiple genomic elements. For example, cell cycle phase or pluripotency potential are cellular properties that are determined by gene regulatory networks and thus cannot be studied with local analyses. \texit{Global} integration is typically (but not always) performed using unsupervised dimensionality reduction approaches that find common modes of variation between molecular layers. Alternatives have been proposed that perform transformations on each data type before merging them into a common similarity network, e.g. using kernel or graph-based approaches \cite{Lanckriet2004, Wang2014}. Nonetheless, both of these approaches have important limitations that will be discussed in this thesis.

Popular examples of global integration methods that have been adapted from the statistical literature are derived as different flavours of matrix factorisation: Principal Component Analysis (PCA), Canonical Correlation Analysis (CCA, implemented in Seurat \cite{Butler2018}), Group Factor Analysis (MOFA \cite{Argelaguet2018,Argelaguet2019}, which is introduced in this thesis), Projected Least Squares (PLS, implemented in DIABLO \cite{Singh2018}) and Non-negative Matrix factorisation (NMF, implemented in LIGER \cite{Welch2019}), among others. Although all of these methods share important similarities, the statistical assumptions underlying each model are heavily dependent on the common coordinate framework adopted. As such, the output of each data integration model has specific assumptions, challenges and diagnostics that must be adopted accordingly.

\subsubsection{Horizontal integration}

Horizontal integration strategies define features as the common anchors in \textit{unmatched} experiments of the same type. This task is faced in most large-scale scRNA projects where data is generated across multiple batches, as uncontrollable differences in the experimental procedure result in systematic deviations in the observed RNA expression (or even cell type composition) across the different batches. If left unaccounted for, these sources of technical variation can mask relevant biological variability and thus complicate the interpretation of the downstream analysis. Horizontal integration is currently the most common integrative task, and it is typically regarded as a batch correction problem, where the aim is to remove undesired technical variation across batches while preserving the biological variation contained within each batch. With the growing availability of reference atlases, epitomised by the Human Cell Atlas project \cite{Aviv2017}, this is arguably one of the most important steps in a single-cell analysis pipeline. 

Linear batch correction methods that were originally developed for bulk datasets (\texit{limma} \cite{Ritchie2015}, \textit{ComBat} \cite{Johnson2006}) are not successful for single-cell experiments, mainly because they assume identical (or at least, known) cell type composition across batches. In practice, however, the abundance of cellular subpopulations can vary even between biological replicates due to subtle differences in the library preparation. As a consequence, the majority of horizontal integration methods developed for single-cell data rely on non-linear (or locally linear) strategies that account for differences in cell type compositions.

Several integrative methods have been developed and benchmarked. This includes MNN \cite{Haghverdi2018}, Seurat \cite{Butler2018}, LIGER \cite{Welch2019}, Harmony \cite{Korsunsky2019}, BBKNN \cite{Polanski2019}, scVI \cite{Lopez2018}, Conos \cite{Barkas2019}, among others, which have been benchmarked in an independent study \cite{Luecken2020}. Despite sharing similar principles, these each employ different methodologies. In particular, MNN and Seurat v3 detect mutual nearest neighbors in a joint low-dimensional space, defined by either principal components (MNN) or canonical covariates (Seurat v3). Instead, LIGER performs integrative NMF and disentangles dataset-specific factors versus shared factors, followed by the construction of a neighborhood graph using the shared factors. Harmony learns a cell-specific linear correction function by successive rounds of k-means clustering on a principal component space. BBKNN performs correction on a neighbourhood graph, which results in much faster computations at the expense of losing single-cell resolution. Finally, scVI is a Bayesian variational autoencoder with a probabilistic formulation which includes random variables that account for batch-specific variation.

Most of these methods also share a common set of challenges. First, a classical problem of non-linear integration methods is over-correction, which occurs when the batch correction vector is wrongly estimated and the algorithm forcibly merges non-matching subpopulations of cells. This can occur for example when no shared axes of biological variation are preserved between the datasets. An optimal method should be able to detect this and prevent a merging of datasets when no biological variation exists. Second, most methods perform data integration in a denoised latent space, typically using principal components or canonical covariates. This step undoubtedly improves most batch correction algorithms, and is extremely useful for some downstream analysis tasks such as manifold embedding (t-SNE or UMAP plots), graph inference and clustering. However, the high-dimensional observations (i.e. the gene expression counts) can be severely distorted as a result of the batch alignment, and other downstream gene-based analyses such as gene marker detection or differential expression analysis can be problematic \cite{Haghverdi2018}.

\subsubsection{Vertical integration}

Vertical integration strategies take advantage of the unambiguous assignment between the molecular profiles in matched multi-modal experiments and thus define cells as anchors between data modalities. This facilitates the detection of co-variation patterns across features and permits two data integration strategies: gene-based \textit{local} analysis as well as cell-based \textit{global} analysis.

\textbf{Local analysis}

In local analyses, the challenge is to distinguish true interactions between features from spurious associations that can result from global confounding effects. To correct for global confounding effects (both technical and biological) affecting the expression phenotype in eQTL mapping, methods such as PCA, PEER \cite{xXX} and MOFA \cite{Argelaguet2018} are often used to identify factors that capture global expression trends, which can be added as covariates in the linear (mixed) model framework. Similarly, the use of a kinship matrix is used to account for global genotype effects that result from population substructure and individual relatedness. Mapping eQTLs using single-cell genomics has led to the identification of cell type-specific eQTL in rare cell populations, which would have been masked using bulk assays \cite{VanDerWijst2018}]. Additionally, \cite{Cuomo2020} combined differentiations of iPSCs across multiple donors and single cell expression profiles to show how eQTL influence expression dynamically along a differentiation trajectory. Single cell eQTL mapping is growing as a field, and it promises to provide an extra layer to our understanding of genetic regulation at the molecular level. As methods to assay various molecular traits at single cell resolution become more established, non-expression single cell QTL mapping, where genomic variants are associated with changes in DNA methylation (mQTL), histone modifications (hQTL) or protein (pQTL) level at single cell resolution may also become routine.

% Local analyses have also been performed between other molecular layers. For example, \cite{Argelaguet2020}] correlated gene expression with matching promoter DNA methylation and chromatin accessibility during mouse gastrulation, revealing that the expression of pluripotency genes is decreased by the induction of a repressive epigenetic landscape. More recently, highly scalable approaches to simultaneously measure chromatin accessibility and gene expression in thousands of  cells [Ma2020] have increased the power to identify cis-regulatory interactions. 

% Similar to eQTL mapping, association analyses between epigenetics and transcriptomics must account for potential confounding factors, to increase power and minimise false positives. A common confounding factor in epigenetics is the sequence context. For example, ATAC peaks with high GC content are associated with higher levels of chromatin accessibility, and DNA methylation regions with high CG density are associated with low DNA methylation levels. To account for this, previous studies have performed hypothesis testing by designing a null distribution for each feature using, as background, a randomly selected set of features with matching sequence context levels. Another confounding factor that is particularly important in single-cell epigenomics is the cell-to-cell differences in global levels, driven for example by differences in Tn5 efficiency or global methylation. Similar to eQTL analyses, cell-centric covariates can be added as covariates in a linear regression framework.


\textbf{Global analysis}

% A key principle of biological data is that at least some of the variation observed in the molecular measurements results from differences in underlying, often unobserved, processes. Such processes, whether driven by biological or technical effects, are manifested by coordinated changes in multiple features and thus could be retrieved by learning useful mathematical representations that can be linked to the latent biological processes.
% When having a single data modality, PCA is the paradigmatic method for global analysis. PCA infers an orthogonal projection of the data onto a low-dimensional space such that the variance explained by the projected data is maximised. The key for the popularity of PCA is its linearity assumption, which ensures that the resulting principal components are simple and interpretable. PCA has also been applied as an integrative method for multi-modal data by performing independent fits in each modality. This approach was attempted in one of the first multi-modal datasets [ANGERMUELLER2017], where scM&T-seq was used to simultaneously profile RNA expression and DNA methylation on 61 embryonic stem cells [Angermueller2017]. The authors found that a small number of PCs derived from mRNA expression displayed significant correlations with PCs derived from DNA methylation, which suggests that some global sources of variation are preserved across data modalities, but a large fraction of the variation is uncorrelated. This simple analysis provides the intuition for some of the more advanced multi-omic integration methods aimed at performing variance decomposition across data modalities.

% An alternative strategy has been to apply PCA after concatenation of the datasets, but this has important limitations when applied to datasets where the features are structured into non-overlapping views (referred to as multi-view data in the machine learning literature). First, PCA extracts components that maximise the variance explained, but it is difficult to quantify the contribution that each component has from each data modality. Second, in its vanilla implementations, PCA does not handle missing values and hence imputation is required when cells do not have a total overlap between data modalities. This is a frequent problem in matched assays, as cells might pass quality control for one data modality but not the other. Third, by maximising the variance explained, PCA implicitly assumes a normal distribution for each feature, and is not well suited for the integration of binary and count-based readouts. 

% Generalisations of PCA for the integration of multi-omics data have been devised by adapting multi-view learning methods from the statistics literature. Although most of these methods were originally devised for bulk data, the majority of them remain applicable to single-cell multi-modal data. This includes MOFA, JIVE, PLS, MCIA, iNMF and MOGSA, among others, which are all unsupervised dimensionality reduction methods derived as different flavours of matrix factorisation. Leaving the differences between these methods aside, their common matrix factorisation framework has been notably successful due to its simplicity, interpretability, speed and limited overfitting. Also, it has shown to be an excellent choice for extracting interpretable signatures from sparse and noisy datasets.


\subsubsection{Diagonal integration}

The third type of data integration problem is faced when no common coordinate framework exists in the high-dimensional space. This task is faced in \textit{unmatched} experiments when different molecular layers are profiled in different subsets of cells. These methods are generally aimed at reconstructing a low-dimensional manifold that captures co-variation across data modalities. Thus, a critical assumption of this integrative strategy is the existence of a latent manifold that is shared between the data modalities. For example, this could represent cells sampled from a common differentiation trajectory or cells sampled from a common set of discrete subpopulations.

The simplest strategy that has been employed to solve a diagonal integration task is to transform it into a simpler horizontal integration task. This can be achieved by summarising molecular measurements over genomic elements that can be unambiguously linked (i.e. RNA expression and promoters methylation). Using this strategy, horizontal methods such as LIGER \cite{Welch2019} and Seurat \cite{Stuart2019b} have been successful at integrating unmatched epigenetic and transcriptomic experiments from the same tissue, and even across different species. However, this strategy relies on fragile biological assumptions and is prone to fail in scenarios where molecular layers are not strongly correlated. A good example is early embryonic development where promoter DNA methylation and/or chromatin accessibility are not as predictive of RNA expression \cite{Argelaguet2019} as in terminally differentiated cell types. 

Alternatively, a few methods have attempted to solve this problem by reconstructing technology-invariant integrated latent spaces. The first method to be developed was MATCHER \cite{Welch2017}, a gaussian process latent variable model. However, this method relies upon the strong assumption that biological variation is defined by a unidimensional axis of variation. More recent methods, including MMD-MA \cite{Liu2019}, SCIM \cite{Stark2020} and UnionCom \cite{Cao2020} have generalised MATCHER to account for complex multivariate trajectories.

MENTION FUTURE FOR THIS TASK....


\section{Thesis overview}

In this PhD thesis I sought to develop computational strategies for data integration in the context of single-cell multi-omics. In particular my research focused on the vertical integration approach using \textit{matched} assays, where cells are the common coordinate framework.

In Chapter 1 I introduce single-cell nucleosome, methylation and transcription sequencing (scNMT-seq), an experimental protocol for the genome-wide profiling of RNA expression, DNA methylation and chromatin accessibility in single cells. While some approaches have reported unbiased genome-wide measurements of up to two molecular layers, scNMT-seq allows, for the first time, the simultaneous profiling of three molecular layers at single cell resolution. We validate the readouts using a simple prototype experiment, and we show how scNMT-seq can be used to study coordinated epigenetic and transcriptomic heterogeneity along a simple differentiation process.

In Chapter 2 I present Multi-Omics Factor Analysis (MOFA), a statistical framework for the integration of multi-omics data sets. MOFA is a latent variable model that offers a principled approach to explore, in a completely unsupervised manner, the underlying sources of sample heterogeneity in a multi-omics data set. Once the model is trained, the inferred low-dimensional space can be interpreted using a tool-kit of downstream analysis procedures that include visualisations, clustering, imputation or prediction of clinical outcomes. First, we validate the different model features using simulated data. Second, we apply MOFA to a multi-omics study of 200 chronic lymphocytic leukaemia patients. In a quick unsupervised analysis, MOFA revealed the most important dimensions of disease heterogeneity, connected to clinical markers that are commonly used in practice. In a second application we show how MOFA can cope with noisy single-cell multi-modal data, identifying coordinated transcriptional and epigenetic changes along a differentiation process.

In Chapter 3 I discuss how we combined scNMT-seq and MOFA to study the role of epigenetic layers during mouse gastrulation, a critical embryonic stage that spans exit from pluripotency to primary germ layer specification. In this study we built the first triple-omics roadmap of mouse gastrulation, which enabled us to perform an integrative study that revealed novel insights on the dynamics of the epigenome. Notably, we show that cells committed to mesoderm and endoderm undergo widespread epigenetic rearrangements, driven by demethylation in enhancer marks and by concerted changes in chromatin accessibility. In contrast, the epigenetic landscape of ectoderm cells remains in a \textit{default} state, resembling earlier stage epiblast cells. This work provides a comprehensive insight into the molecular logic for a hierarchical emergence of the primary germ layers, revealing underlying molecular constituents of the Waddington's landscape.

In Chapter 4 I propose an improved formulation of the MOFA framework aimed at performing integrative analysis of large-scale (single-cell) data sets across multiple studies/conditions as well as data modalities. We introduce key methodological developments, including a fast stochastic variational inference framework and multi-group generalisation in the structure of the prior distributions. All together, this allows MOFA to  disentangle heterogeneity across sample groups (i.e. studies or experimental conditions) and data modalities (i.e. omics) in very large single-cell studies. First, we benchmark the new features of the model using simulated data. Next, we use a single-cell DNA methylation data set of neurons from mouse frontal cortex to demonstrate how from a seemingly unimodal data set, one can investigate hypothesis using a multi-group and multi-view setting. Finally, we apply MOFA to the scNMT-seq data set generated in Chapter 3, revealing underlying sources of molecular variation associated with early cell fate decisions.

Finally, Chapter 5 summarises this thesis and provides an outlook of future research.
