\graphicspath{{Introduction/Figs/}}

\chapter{Overview}

\section{Profiling multi-omics at single-cell resolution}

Next-generation sequencing technologies have revolutionised the study of biological systems by enabling the genome-wide profiling of molecular layers in un unbiased manner, including the genome \cite{Fleischmann1995} the epigenome \cite{Frommer1992} and the transcriptome \cite{Lister2008,Bainbridge2006,Nagalakshmi2008,Mortazavi2008}, among others. However, bulk sequencing approaches rely on a large number of cells to report an average molecular readout, and are hence limited for the study of complex biological processes where heterogeneity is expected at single cell resolution \cite{Griffiths2018,Papalexi2017,Patel2014}. The progressive development of low-input sequencing techniques resulted in an explosion of single-cell sequencing technologies, mostly for the transcriptome. In contrast to bulk protocols, single-cell techniques provide an unprecedented opportunity to study the molecular variation associated with cellular heterogeneity, lineage diversification and cell fate commitment \cite{Kolodziejczyk2015}.

The field of single-cell sequencing has largely been driven by the quantification of the messenger RNA (mRNA). In less than a decade, the field of single-cell transcriptomics has experienced an exponential growth of scale, driven by incremental optimisations of reagent volumes and consumable costs, as well as profound changes in the nature of the technology \cite{Svensson2018}. The earliest high-throughput scRNA-seq technologies were published between 2009 and 2011, yielding a handful of cells. In 2019, there are studies that have achieved the astonishing milestone of profiling the transcriptome for more than a million cells in a single experiment \cite{Cao2019}. With the development of efficient commercial platforms, the maturation of scRNA-sequencing technologies has provided major insights on the study of lineage diversification and cell fate commitment \cite{Kolodziejczyk2015,Griffiths2018,Papalexi2017,Patel2014}. In 2020, we are at the stage of a major endeavour to generate transcriptomic atlases for different tissues, embryos and even entire adult organisms. The most ambitious of all is the Human Cell Atlas, aimed at building a reference map for all cells in the human body \cite{Aviv2017}.

While the large majority of single-cell studies are focused on capturing RNA expression information, transcriptomic readouts provide a single dimension of cellular heterogeneity and hence contain limited information to characterise the molecular determinants of phenotypic variation \cite{Ritchie2015}. Consequently, gene expression markers have been identified for a myriad of biological systems, but the role of the accompanying epigenetic changes in driving cell fate decisions remains poorly understood \cite{Griffiths2018,Kelsey2017,Bheda2014}. Thus, to get a better insight into the epigenetics of cell fate commitment, significant effort has been placed to obtain epigenetic measurements at single-cell resolution by adapting bulk methods to low-input material. This has been particularly successful for chromatin accessibility. Due to its cost-effective strategy, single-cell ATAC-seq (scATAC-seq) has become the most popular technique to map open chromatin, and is also available in an efficient commercial platform \cite{Cusanovich2015,Cao2018,Chen2018}.\\
Other molecular layers have also been queried, including DNA methylation \cite{Smallwood2014}, histone modifications \cite{Ku2019}, chromatin conformation \cite{Ku2019}, proteomics \cite{Specht2018} and lipidomics \cite{Thiele2019}.

Despite its success in studying molecular variation, no single "-omics" technology can capture the intricacy of complex biological mechanisms. Nonetheless, the collective information has the potential to draw a more comprehensive picture of biological processes \cite{Hasin2017,Ritchie2015}. In particular, multi-omics (or multi-modal) assays have the potential to go beyond snapshots and provide a more dynamic, perhaps even mechanistic, understanding of the connection between molecular layers. Motivated by this, multi-omic data sets are receiving increasing interest across a wide range of biological domains, including cancer biology \cite{Akavia2010,Gerstung2015}, regulatory genomics \cite{Chen2016}, microbiology \cite{Kim2016} or host-pathogen interactions \cite{Soderholm2016}. 

The profiling of multi-omic readouts at the bulk level is relatively simple, as the same tissue can be dissociated into different aliquots, where each assay can be performed independently \cite{Ritchie2015}. This strategy is also used with single-cell assays, but it has the important downside that the different molecular layers cannot be unambiguously linked, hence limiting the insights that can be inferred from the data. The ultimate goal in single-cell sequencing is to obtain multiple molecular readouts from the same cell. Matched multi-modal measurements can be obtained using a variety of strategies, some of which will be discussed in this thesis. The development of these technologies will help us understand the fundamental regulatory principles that connect the different molecular layers. In addition, integrative analyses that simultaneously pool information across multiple data modalities (-omics) and across multiple studies promise to deliver a more comprehensive insights into the complex variation that underlies cellular populations \cite{Stuart2019,Colome-Tatche2018}. An overview of such technologies will be provided in Chapter 1.

Notably, the early success and rapid development of single-cell multi-modal methods has led to their recognition as Method of the year in 2019 by the journal \textit{Nature Methods} \cite{NatMethods2020}. However, their development is still in pilot stages and there is no commercial platform available, limiting its widespread use by the community. Furthermore, common challenges in (uni-modal) single-cell assays such as low coverage and high levels of technical noise become exacerbated when doing multi-modal profiling. Quoting Cole Trapnell, one of the pioneers of single-cell data analysis: \textit{When you do a multi-omic assay, you're combining all the bad things from multiple protocols} \cite{Eisenstein2020}. Thus, one of the biggest challenge in integrative multi-modal analysis is to develop statistical frameworks that are capable of uncovering biological signal across multiple data modalities while overcoming the technical biases and missing information that are inherent to single-cell experiments. 

\section{Single-cell data analysis}

From the computational perspective, the rapid development of single-cell technologies has introduced unprecedented challenges for the statistical community, and novel computational methods need to be developed (or adapted) for interrogating the data generated \cite{Stegle2015}. The vast majority of methods are focused on RNA expression, spanning multiple tasks that include normalisation \cite{Lun2016a}, feature selection \cite{Townes2019}, differential expression \cite{Kharchenko2014}, clustering \cite{Kiselev2017}, cell type recognition \cite{Abdelaal2019}, pseudotime inference \cite{Haghverdi2016}, detection of gene regulatory networks \cite{Aibar2017} and batch correction \cite{Haghverdi2018}, among others. Analysis tools have been wrapped into popular platforms such as Seurat \cite{Butler2018}, the Bioconductor class \textit{SingleCellExperiment} \cite{Amezquita2020} and Scanpy \cite{Wolf2018}.

In this section I will provide a brief overview of a typical scRNA-seq analysis pipeline, paying particular attention to the methods that I have used throughout this thesis.

\subsection{Read alignment and gene expression quantification}

The first step in the computational pipeline is to demultiplex the DNA barcodes in order to identify reads that originate from the same cell. This is particularly important when multiple experiments are pooled into a common sequencing library. This task is significantly more complex than in bulk data, owing to the large number of cells and the high rates of errors that can introduce nucleotide missmatches \cite{Tambe2019}.

Subsequently, trimmed reads are aligned to the appropriate reference transcriptome. Gene expression is represented as an integer matrix of counts, with rows representing genomic features (typically genes) and the columns representing individual cells. 

\subsection{Quality control}

Incomplete cell lysis or failures during library preparation may result in poor quality cells that need to be removed for a successful downstream analysis. Typical quality control metrics are the total number of reads detected per cell, the number of genes expressed and the fraction of mitochondrial genes. Cells that are outliers for some of these metrics are filtered out. Importantly, even though there is a generic strategy to assess the quality control for scRNA-seq samples, the specific thresholds vary between data sets and technologies, and care must be taken to always visualise the quality control metrics \cite{Luecken2019}.

A common source of technical variability in single-cell experiments is the existence of doublets, which occurs when more than one cell colocate in the same well or in the same droplet and are thus assigned the same cell barcode. This results in cells that appear as mixtures of different cellular populations and can be mistaken for non-existing intermediate populations or transitory states.  Thus, it is important to remove doublets so that they do not compromise the downstream analysis. In small-scale plate-based technologies, most doublets can be excluded simply by microscope inspection, but in large-scale droplet-based technologies one needs to adopt data-driven heuristics to exclude multiplet libraries \cite{McGinnis2019}. 


\subsection{Normalisation}

The first step of quality control is essential to remove poor quality cells, but the quality control metrics can vary widely even between cells that pass the filtering criteria. These sources of technical variation arise from any of the library preparation steps, and includes PCR amplification biases, differences in RNA capture and reverse transcription efficiency, among others. In addition, the stochasticity of the amplification process produces dropout events, in which no read counts are observed for genes that are expressed \cite{VandenBerge2018}. This does not tend to be a problem for highly-expressed genes, but it poses a major challenge for lowly expressed genes.

Data normalisation steps are mandatory to eliminate (or at least reduce) the technical variation. Methods that were developed for bulk RNA-seq, including \textit{TMM} \cite{Robinson2010} and \textit{DEseq2} \cite{Love2014} are not successful for scRNA-seq owing to large number of zeros that dominate the gene expression matrix.

In this thesis I adopted the methodology implemented in the \textit{scran} package \cite{Lun2016a}. Briefly, this normalisation procedure divides the gene counts by a size factor per cell and subsequently applies a log transformation with pseudocount on each observation. The essential innovation for single-cell data is to pool expression values from multiple cells (resulting in less zeros) and subsequently deconvolves the cell-specific size factors using a linear system of equations.

Recent works have suggested that global size factors do not effectively normalise all genes at the same time, and different groups of genes require specific size factors in order to remove technical biases \cite{Hafemeister2019}. In this thesis I have not explored this approach, but it showcases how data normalisation is still an open and debated topic in scRNA-seq.

\subsection{Dimensionality reduction}

A key principle of biological data sets is that covariation patterns between the features (i.e. genes) results from differences in underlying processes that can be inferred and interpreted. This key assumption sets off an entire statistical framework of exploiting the redundancy encoded in the data set to reduce the dimensionality of the data in an unsupervised fashion.

Principal Component Analysis (PCA) is the most popular technique for dimensionality reduction of scRNA-seq data \cite{Luecken2019}. A typical analysis pipeline performs clustering, graph inference and other downstream analyses on the (denoised) latent PCA space defined by the top N principal components (where components are ranked by variance explained). Importantly, by maximising the variance explained, PCA implicitly assumes a normal distribution for each feature, and it therefore requires the log transformation above to turn integer counts into continuous measurements. In addition, the log transformation prevents signal being driven by a small number of extremely highly-expressed genes (because in the raw counts the variance of each gene is proportional to its mean expression). 

PCA defines a linear transformation from the high-dimensional space to the low-dimensional space where each component captures an orthogonal source of variation. Capturing the biological signal in most single-cell data sets require a relatively high number of components. Unfortunately, humans do not have the ability to make visual representations of more than three dimensions at the same time, so further dimensionality reduction is typically applied using non-linear techniques, including t-Distributed Stochastic Neighbor Embedding (t-SNE) \cite{vanDerMaaten2008} and Uniform Manifold Approximation and Projection (UMAP) \cite{McInnes2018}. Both methods have been extensively applied, although UMAP is gaining popularity for larger data sets because it preserves better the global structure, whereas t-SNE is largely aimed at preserving local structure. 

\subsection{Clustering}

Unsupervised clustering is arguably one of the most powerful application of single-cell genomics, as it underpins the ability to define cell types in a coherent, systematic and unbiased manner. Although clustering is still largely empirical and no strong consensus exists on the methodology and the parameters, it is applied in virtually any single-cell data set in one form or another \cite{Kiselev2019}. The most popular clustering algorithm has traditionally been k-means, which iteratively identifies $k$ cluster centroids, and assigns each cell to the nearest centroid. This method is simple, fast and efficient for medium-sized data sets. For large-scale data sets, however, the use of community-detection algorithms on coarse-grained graphs has become more popular. Briefly, the first step of commnity-detection methods is to build a k-nearest-neigbourhood graph using a cell-to-cell similarity metric, where each node corresponds to one cell. Then, tighly connected communities are detected by maximising a modularity score, where the modularity quantifies the assignment of nodes to communities when contrasted to a random network.

\subsection{Inference of developmental trajectories}

In many biological systems, and particularly during embryonic development, cells display a continuous spectrum of states where discretisation by clustering may be inappropriate. Due to the destructive nature of single-cell assays, experiments are limited to snapshots and thus are not capable of measuring continuous time. However, differentiating cells are typically asynchronised and display a continuous spectrum of molecular states that reflect the underlying trajectory. Computational methods have been developed to reconstruct this continuity using latent mathematical representations, and are often called pseudotime methods \cite{Saelens2019}. The aim of pseudotime methods is to generate an ordination of cells according to some metric, which is usually (but not necessarily) some approximation of real time that is inferred from the data. A myriad of pseudotime methods have been developed, with tailored assumptions depending on the nature of the input data and the expected topology of the trajectory (linear, bifurcating, etc.), among other variables \cite{Saelens2019}. 

\section{Integrative analysis}

Despite the explosion of statistical methods for scRNA-seq data analysis, to date only a few methods have been published with the aim of performing data integration across experiments and data modalities. This is currently defined as one of the grand challenges in single-cell data science \cite{Lahnemann2020}.

\subsection{Defining the common coordinate framework}

The first step when performing data integration is to consider a common coordinate framework to anchor the different data modalities. This defines three broad types of strategies for data integration (\Cref{fig:overview_data_integration}): 

\begin{itemize}

	\item Cells as the common coordinate (vertical integration): when the different data modalities are derived from the same cell in \textit{matched} multi-omic assays. This is exemplified by assays such as single-cell Methylome \& Transcriptome (scM\&T-seq) \cite{Angermueller2016}, Cellular Indexing of Transcriptomes and Epitopes by sequencing (CITE-seq) \cite{Stoeckius2017} or Single-nucleus chromatin accessibility and RNA expression sequencing (SHARE-seq) \cite{Ma2020}.

	\item Genomic features as the common coordinate framework (horizontal integration): when multiple data modalities of the same type are derived from different subsets of cells. We call this \textit{non-matched} multi-omics and the main advantage is that it is significantly easier and cheaper to obtain than \textit{matched} multi-omics, and as a result most of the current data sets to date belong to this category. An example could be the profiling of scRNA-seq experiments from cells from the same tissue across different groups of donors, where the set of genes represents the anchors.

	\item No common coordinate framework in the high-dimensional space (diagonal integration): when both cells and genomic features are different between experiments. This is exemplified by profiling RNA expression and chromatin accessibility from independent sets of cells.

\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{overview_data_integration}
	\caption{\textbf{Defining the data integration strategy: choosing the common coordinate framework}.\\Schematic representation of (a) Horizontal integration, when features act as anchors (b) Vertical integration, when cells act as anchors (c) Diagonal integration, when no anchors exist. }
	\label{fig:overview_data_integration}
\end{figure}

\subsection{Defining the methodology}

Once the common coordinate framework is defined, one needs to choose the data integration strategy. These can fall into two classes: \textit{local} and \textit{global}, a notation inspired from integrative approaches that have been pursued at the bulk level \cite{Ritchie2015}. 

\textit{Local} analyses refer to associations between specific features across different molecular layers, with the aim of detecting putative interactions between them. Prominent examples are associations between genetic variants and gene expression (expression quantitative trait loci, eQTL) or correlations between the epigenetic status of putative regulatory elements and gene expression of nearby genes. The restriction to a local search space is often necessary, to maintain the problem tractable. For example, cis eQTL mapping is most prevalent because by testing only proximal genetic variants for each gene, the effects of the multiple testing burden is reduced \cite{Nica2013}. Since such association analyses are typically performed per feature (across cells), \textit{local} analyses generally requires unambiguous matching between the modalities, as given only by \textit{matched} multi-modal assays, and thus belongs to the category of vertical analysis. As for methods, most \textit{local} analyses rely on different flavours of linear regression models, with different modelling assumptions depending on the nature of the molecular readouts. This can include non-gaussian likelihoods, sparsity assumptions to prevent overfitting or probabilistic terms to account for random effects. For example, linear mixed models (LMMs) are a popular framework for performing genetic analyses \cite{Moore2019}. In a LMM, a random effect term is added to account for the population structure and relatedness between individuals, which may affect both the phenotype and the genotype, thus leading to spurious associations if not accounted for.

While useful for characterising genetic variants or identifying putative regulatory elements, \textit{local} analyses have limited capacity to discover complex maps of molecular heterogeneity that result from interactions between genomic features. An alternative strategy for data integration is to exploit the full spectrum of measurements to identify cellular states defined by the coordinated action of multiple genomic elements. For example, cell cycle phase or pluripotency potential are cellular properties that are determined by gene regulatory networks and thus cannot be studied with \textit{local} analyses. \textit{Global} integration is typically (but not always) performed using unsupervised dimensionality reduction approaches that find common modes of variation between molecular layers. Alternatives have been proposed that perform transformations on each data type before merging them into a common similarity network, e.g. using kernel or graph-based approaches \cite{Lanckriet2004, Wang2014}. Nonetheless, both of these approaches have important limitations that will be discussed in this thesis.

Popular examples of global integration methods that have been adapted from the statistical literature are derived as different flavours of matrix factorisation: PCA, Canonical Correlation Analysis (CCA, implemented in Seurat \cite{Butler2018}), Group Factor Analysis (MOFA \cite{Argelaguet2018,Argelaguet2019}, which is introduced in this thesis), Projected Least Squares (PLS, implemented in DIABLO \cite{Singh2018}) and Non-negative Matrix factorisation (NMF, implemented in LIGER \cite{Welch2019}), among others. Although all of these methods share important similarities, the statistical assumptions underlying each model are heavily dependent on the common coordinate framework adopted. As such, the output of each data integration model has specific assumptions, challenges and diagnostics that must be adopted accordingly.

\subsubsection{Horizontal integration}

Horizontal integration strategies define features as the common anchors in \textit{unmatched} experiments of the same type. This task is faced in most large-scale scRNA projects where data is generated across multiple batches, as uncontrollable differences in the experimental procedure result in systematic deviations in the observed RNA expression (or even cell type composition) across the different batches. If left unaccounted for, these sources of technical variation can mask relevant biological variability and thus complicate the interpretation of the downstream analysis. Horizontal integration is currently the most common integrative task, and it is typically regarded as a batch correction problem, where the aim is to remove undesired technical variation across batches while preserving the biological variation contained within each batch \cite{Tran2020}. With the growing availability of reference atlases, epitomised by the Human Cell Atlas project \cite{Aviv2017}, this is arguably one of the most important steps in a single-cell analysis pipeline. 

Linear batch correction methods that were originally developed for bulk datasets such as \textit{limma} \cite{Ritchie2015b} and \textit{ComBat} \cite{Johnson2006}) are not successful for single-cell experiments, mainly because they assume identical (or at least, known) cell type composition across batches. In practice, however, the abundance of cellular subpopulations can vary even between biological replicates due to subtle differences in the library preparation. As a consequence, the majority of horizontal integration methods developed for single-cell data rely on non-linear (or locally linear) strategies that account for differences in cell type compositions.

Several integrative methods have been developed and benchmarked. This includes MNN \cite{Haghverdi2018}, Seurat \cite{Butler2018}, LIGER \cite{Welch2019}, Harmony \cite{Korsunsky2019}, BBKNN \cite{Polanski2019}, scVI \cite{Lopez2018}, Conos \cite{Barkas2019}, among others, which have been benchmarked in an independent study \cite{Luecken2020}. Despite sharing similar principles, these each employ different methodologies. In particular, MNN and Seurat v3 detect mutual nearest neighbors in a joint low-dimensional space, defined by either principal components (MNN) or canonical covariates (Seurat v3). Instead, LIGER performs integrative NMF and disentangles dataset-specific factors versus shared factors, followed by the construction of a neighborhood graph using the shared factors. Harmony learns a cell-specific linear correction function by successive rounds of k-means clustering on a principal component space. BBKNN performs correction on a neighbourhood graph, which results in much faster computations at the expense of losing single-cell resolution. Finally, scVI is a Bayesian variational autoencoder with a probabilistic formulation which includes random variables that account for batch-specific variation.

Most of these methods also share a common set of challenges. First, a classical problem of non-linear integration methods is over-correction, which occurs when the batch correction vector is wrongly estimated and the algorithm forcibly merges non-matching subpopulations of cells \cite{Luecken2020}. This can occur for example when no shared axes of biological variation are preserved between the datasets. An optimal method should be able to detect this and prevent a merging of datasets when no biological variation exists. Second, most methods perform data integration in a denoised latent space, typically using principal components or canonical covariates. This step undoubtedly improves most batch correction algorithms, and is extremely useful for some downstream analysis tasks such as manifold embedding (t-SNE or UMAP plots), graph inference and clustering. However, the high-dimensional observations (i.e. the gene expression counts) can be severely distorted as a result of the batch alignment, and other downstream gene-based analyses such as gene marker detection or differential expression analysis can be problematic \cite{Haghverdi2018}.

\subsubsection{Vertical integration}

Vertical integration strategies take advantage of the unambiguous assignment between the molecular profiles in \textit{matched} multi-modal experiments and thus define cells as anchors between data modalities. This facilitates the detection of co-variation patterns across features and permits two data integration strategies: gene-based \textit{local} analysis and a cell-based \textit{global} analysis.

\textbf{Local analysis}

In \textit{local} analyses, the challenge is to distinguish true interactions between features from spurious associations that can result from global confounding effects. To correct for global confounding effects (both technical and biological) affecting the expression phenotype in eQTL mapping, methods such as Principal Component Analysis and PEER \cite{Stegle2010} are often used to identify factors that capture global expression trends, which can be added as covariates in the linear (mixed) model framework. Similarly, the use of a kinship matrix is used to account for global genotype effects that result from population substructure and individual relatedness. Mapping eQTLs using single-cell genomics has led to the identification of cell type-specific eQTL in rare cell populations, which would have been masked using bulk assays \cite{VanDerWijst2018}. Additionally, \cite{Cuomo2020} combined differentiations of iPSCs across multiple donors and single cell expression profiles to show how eQTL influence expression dynamically along a differentiation trajectory. Single cell eQTL mapping is growing as a field, and it promises to provide an extra layer to our understanding of genetic regulation at the molecular level. As methods to assay various molecular traits at single cell resolution become more established, non-expression single cell QTL mapping, where genomic variants are associated with changes in DNA methylation (mQTL), histone modifications (hQTL) or protein (pQTL) level at single cell resolution may also become routine.

\textbf{Global analysis}

% A key principle of biological data is that at least some of the variation observed in the molecular measurements results from differences in underlying, often unobserved, processes. Such processes, whether driven by biological or technical effects, are manifested by coordinated changes in multiple features and thus could be retrieved by learning useful mathematical representations that can be linked to the latent biological processes.

When having a single data modality, PCA is the paradigmatic method for global analysis, and will be discussed in more detail in Chapter 2. Briefly, PCA infers an orthogonal projection of the data onto a low-dimensional space such that the variance explained by the projected data is maximised. The key for the popularity of PCA is its linearity assumption, which ensures that the resulting principal components are simple and interpretable. PCA has also been applied as an integrative method for multi-modal data by extracting principal components from each modality and subsequently comparing them. This approach was attempted in one of the first multi-modal datasets, where scM\&T-seq was used to simultaneously profile RNA expression and DNA methylation on 61 embryonic stem cells \cite{Angermueller2016}. The authors found that a small number of PCs derived from mRNA expression displayed significant correlations with PCs derived from DNA methylation, which suggests that some global sources of variation are preserved across data modalities, but a large fraction of the variation is uncorrelated. This simple analysis provides the intuition for some of the more advanced multi-omic integration methods aimed at performing variance decomposition across data modalities.

An alternative strategy has been to apply PCA after concatenation of the datasets, but this has important limitations when applied to datasets where the features are structured into non-overlapping views (referred to as multi-view data in the machine learning literature). First, PCA extracts components that maximise the variance explained, but it is difficult to quantify the contribution that each component has from each data modality. Second, in its vanilla implementation, PCA does not handle missing values and hence imputation is required when cells do not have measurements available in all data modalities. This is a frequent problem in \textit{matched assays}, as cells might pass quality control for one data modality but not the other. Third, by maximising the variance explained, PCA implicitly assumes a normal distribution for each feature, and is not well suited for the integration of binary and count-based readouts. 

Generalisations of PCA for the integration of multi-omics data have been devised by adapting multi-view learning methods from the statistics literature. Although most of these methods were originally devised for bulk data, the majority of them remain applicable to single-cell multi-modal data. This includes MOFA \cite{Argelaguet2018}, JIVE \cite{Lock2013}, PLS \cite{Singh2018}, MCIA \cite{Meng2014} and iNMF \cite{Welch2019}, all being unsupervised dimensionality reduction methods derived as different flavours of matrix factorisation. As I will discuss in this thesis, the matrix factorisation framework is very appealing due to its simplicity, interpretability, scalability and limited overfitting. Also, it has shown to be an excellent choice for extracting interpretable signatures from sparse and noisy observations such as single-cell measurements.


\subsubsection{Diagonal integration}

The third type of data integration problem is faced when no common coordinate framework exists in the high-dimensional space. This task is faced in \textit{unmatched} experiments when different molecular layers are profiled in different subsets of cells, and is arguably the hardest type of integration. Diagonal integration methods are generally aimed at reconstructing a low-dimensional manifold that captures co-variation across data modalities. Thus, a critical assumption of this integrative strategy is the existence of a latent manifold that is shared between the data modalities. For example, this could represent cells sampled from a common differentiation trajectory or cells sampled from a common set of discrete subpopulations.

The simplest strategy that has been employed to solve a diagonal integration task is to transform it into a simpler horizontal integration task. This can be achieved by summarising molecular measurements over genomic elements that can be unambiguously linked (i.e. RNA expression and promoters methylation). Using this strategy, horizontal methods such as LIGER \cite{Welch2019} and Seurat \cite{Stuart2019b} have been successful at integrating unmatched epigenetic and transcriptomic experiments from the same tissue, and even across different species. However, this strategy relies on fragile biological assumptions and is prone to fail in scenarios where molecular layers are not strongly correlated. A good example is early embryonic development where promoter DNA methylation and/or chromatin accessibility are not as predictive of RNA expression \cite{Argelaguet2019} as in terminally differentiated cell types. 

Alternatively, a few methods have attempted to solve this problem by reconstructing technology-invariant integrated latent spaces. The first method to be developed was MATCHER \cite{Welch2017}, a gaussian process latent variable model. However, this method relies upon the strong assumption that biological variation is defined by a unidimensional axis of variation. Some recent methods, including MMD-MA \cite{Liu2019a}, SCIM \cite{Stark2020} and UnionCom \cite{Cao2020} have generalised MATCHER to account for complex multivariate trajectories. However, no independent benchmarking has yet been performed, and the biological insights extracted from these methods have been relatively limited. 


\section{Thesis overview}

In this PhD thesis I sought to develop computational strategies for data integration in the context of single-cell multi-omics. In particular my research focused on the vertical integration task, where cells are the common coordinate framework in \textit{matched} assays.

In Chapter 1 I introduce single-cell nucleosome, methylation and transcription sequencing (scNMT-seq), an experimental protocol for the genome-wide profiling of RNA expression, DNA methylation and chromatin accessibility in single cells. While some approaches have reported unbiased genome-wide measurements of up to two molecular layers, scNMT-seq allows, for the first time, the simultaneous profiling of three molecular layers at single cell resolution. We validate the readouts using a simple prototype experiment, and we show how scNMT-seq can be used to study coordinated epigenetic and transcriptomic heterogeneity along a simple differentiation process.

In Chapter 2 I present Multi-Omics Factor Analysis (MOFA), a statistical framework for the integration of multi-omics data sets. MOFA is a latent variable model that offers a principled approach to explore, in a completely unsupervised manner, the underlying sources of sample heterogeneity in a multi-omics data set. Once the model is trained, the inferred low-dimensional space can be interpreted using a tool-kit of downstream analysis procedures that include visualisations, clustering, imputation or prediction of clinical outcomes. First, we validate the different model features using simulated data. Second, we apply MOFA to a multi-omics study of 200 chronic lymphocytic leukaemia patients. In a quick unsupervised analysis, MOFA revealed the most important dimensions of disease heterogeneity, connected to clinical markers that are commonly used in practice. In a second application we show how MOFA can cope with noisy single-cell multi-modal data, identifying coordinated transcriptional and epigenetic changes along a differentiation process.

In Chapter 3 I discuss how we combined scNMT-seq and MOFA to study the role of epigenetic layers during mouse gastrulation, a critical embryonic stage that spans exit from pluripotency to primary germ layer specification. In this study we built the first triple-omics roadmap of mouse gastrulation, which enabled us to perform an integrative study that revealed novel insights on the dynamics of the epigenome. Notably, we show that cells committed to mesoderm and endoderm undergo widespread epigenetic rearrangements, driven by demethylation in enhancer marks and by concerted changes in chromatin accessibility. In contrast, the epigenetic landscape of ectoderm cells remains in a \textit{default} state, resembling earlier stage epiblast cells. This work provides a comprehensive insight into the molecular logic for a hierarchical emergence of the primary germ layers, revealing underlying molecular constituents of the Waddington's landscape.

In Chapter 4 I propose an improved formulation of the MOFA framework aimed at performing integrative analysis of large-scale (single-cell) data sets across multiple studies/conditions as well as data modalities. We introduce key methodological developments, including a fast stochastic variational inference framework and multi-group generalisation in the structure of the prior distributions. All together, this allows MOFA to  disentangle heterogeneity across sample groups (i.e. studies or experimental conditions) and data modalities (i.e. omics) in very large single-cell studies. First, we benchmark the new features of the model using simulated data. Next, we use a single-cell DNA methylation data set of neurons from mouse frontal cortex to demonstrate how from a seemingly unimodal data set, one can investigate hypothesis using a multi-group and multi-view setting. Finally, we apply MOFA to the scNMT-seq data set generated in Chapter 3, revealing underlying sources of molecular variation associated with early cell fate decisions.

Finally, Chapter 5 summarises this thesis and provides an outlook of future research.
