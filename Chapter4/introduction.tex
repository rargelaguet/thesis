\graphicspath{{Chapter4/Figs/simulations/}{Chapter4/Figs/scrna/}{Chapter4/Figs/scmet/}{Chapter4/Figs/scnmt/}}

\chapter{MOFA+: an improved framework for the comprehensive integration of structured single-cell data}

In Chapter 2 we developed Multi-Omics Factor Analysis (MOFA), a statistical framework for the unsupervised integration of multi-modal data. \\
MOFA addresses key challenges in data integration, including overfitting, noise reduction, handling of missing values and improved interpretation of the model output. Hower, when applied to increasingly-large (single-cell) data sets, the inference scheme implemented in MOFA is still limited in scalability. \\
In addition to the increase in the number of cells, the increased experimental throughput has facilitated the study of larger numbers of experimental conditions. MOFA makes strong assumptions about the dependencies across samples and it hence has no principled way of modelling data sets where the samples are structured into multiple groups, where groups can be defined as batches, donors or different experiments. By pooling and contrasting information across studies or experimental conditions, it would be possible to obtain more comprehensive insights into the complexity underlying biological systems.

In this new Chapter we improve the first model formulation with the aim of performing integrative analysis of large-scale datasets simultaneously across multiple data modalities and across multiple groups.


\section{Theoretical fundations}

\subsection{Exponential family distributions} \label{section:exponential_family}

Exponential family distributions are a parametric class of probability distributions that have a characteristic set of mathematical properties which make them amenable for probabilistic modelling.\\
Most of the commonly used probability distributions belong to the exponential family, including the normal or Gaussian, gamma, Poisson, bernoulli, exponential, etc.

Exponential family distributions can be summarised in the following form:
\begin{equation}
	p(\bfx|\btheta) = h(\bfx) \exp \{ \eta(\btheta) T(\bfx) - A(\btheta) \}
\end{equation}
where $\bfx$ is a multivariate random variable and $\btheta$ are the distribution's parameters. Each one of the term has a common notation:
\begin{itemize}
	\item T(\bfx): the sufficient statistics.
	\item $\eta(\btheta)$: the natural parameters.
	\item $h(\bfx)$: the base mesure.
	\item $A(\eta)$: the log-partition function (or the normaliser).
\end{itemize}
In the context of Bayesian inference, the major property that make exponential family distributions useful for statistical modelling is that they have conjugate priors. Just as a reminder for the reader, a conjugate prior is a combination of likelihood and prior distributions which ensure a closed-form posterior distribution which is of the same form as the prior. As we have discussed in Chapter 2, this property is crucial for enabling efficient statistical inference, otherwise posterior distributions must be computed using expensive and approximate numerical methods.

The exponential family form for the probability distributions frequently used in this thesis are the following:

Univariate normal distribution:
\begin{align*}
	& \eta(\mu,\sigma) = \lbrack \frac{\mu}{\sigma^2}; -\frac{1}{2\sigma^2} \rbrack \\
	& h(x) = \frac{1}{\sqrt{2\pi}} \\
	& T(x) = \lbrack x; x^2 \rbrack \\
	& A(\mu,\sigma) = \frac{\mu^2}{2\sigma^2} + \log \| \sigma \|
\end{align*}

Multivariate normal distribution:
\begin{align*}
	& \eta(\bmu,\bSigma)  = \lbrack\Sigma^{-1} \mu; -0.5\Sigma^{-1} \rbrack \\
	& T(x) = \lbrack x; xx^T \rbrack \\
	& h(x) = (2\pi)^{-\frac{k}{2}} \\
	& A(\theta) = -0.25\eta_1^T \eta_2{-1} \eta_1 - 0.5\log(\|-2\eta_2\|)
\end{align*}

Gamma distribution:
\begin{align*}
	& \eta = \lbrack \alpha - 1; -\beta \rbrack \\
	& T(x) = \lbrack \log x; x \rbrack \\
	& h(x) = 1 \\
	& A(\theta) = \log(\Gamma(\eta_1 + 1)) - (\eta_1 + 1) \log(-\eta_2)
\end{align*}

Beta distribution:
\begin{align*}
	& \eta = [\alpha; \beta] \\
	& T(x) = [\log x; \log (1-x)] \\
	& h(x) = \frac{1}{x(1-x)} \\
	& A(\theta) = \log(\Gamma(\eta_1)) +\log(\Gamma(\eta_2)) - \log(\Gamma(\eta_1+\eta_2))
\end{align*}


\subsection{Gradient ascent} \label{section:gradient_ascent}

Gradient ascent is a first-order optimization algorithm for finding the (local) maximum of a function \cite{Bishop2006,Murphy}. Formally, for a differentiable function $F(x)$, the iterative scheme of gradient ascent is:
\begin{equation} \label{gradient_ascent}
	\bfx^{(t+1)} = \bfx^{(t)} + \rho^{(t)} \nabla F(\bfx^{(t)})
\end{equation}
In short, it works by taking steps proportional to the gradient $\nabla F$ evaluated at each iteration $t$. This leads to a monotonic sequence:
\[
	\bfx^{0} \leq \bfx^{1} \leq \bfx^{1} \cdots 
\]
Importantly, the step size $\rho^{(t)}$ is typically adjusted at each iteration $t$ such that it satisfies the Robbins-Monro conditions: $\sum_t \rho^{(t)} = \infty \text{ and } \sum_t (\rho^{(t)})^2 < \infty$. Then $F$ is guaranteed to converge to the global maximum \cite{Robbins-Monro1951} if the objective function is convex. If $F$ is not convex, the algorithm is sensible to the initialisation $\bfx^{t=0}$ and can converge to local maxima instead of the global maximum.


\subsubsection{Stochastic gradient ascent} \label{section:stochastic_gradient_ascent}

Gradient ascent becomes prohibitively slow with large datasets, mainly because of the computational cost involved in the iterative calculation of gradients \cite{Spall2003}.\\
A simple strategy to speed up gradient descent is to replace the actual gradient $\nabla F$ by an estimate $\hat{\nabla} F$ using a randomly selected subset of the data (minibatch).
The iterative scheme is then defined in the same way as in standard gradient ascent:
\begin{equation}
	\bfx^{(t+1)} = \bfx^{(t)} + \rho^{(t)} \hat{\nabla} F(\bfx^{(t)})
\end{equation}

%In practice, the stochastic nature of the algorithm makes the optimisation trajectory more wiggly and typically requires a larger number of iterations than standard gradient ascent. However, the reduced computational cost in computing the gradients yields an overall faster training time

% Copied from hoffman
% Under the right conditions, stochastic optimization algorithms provably converge to an optimum of the objective. Stochastic optimization is particularly attractive when the objective (and therefore its gradient) is a sum of many terms that can be computed independently. In that setting, we can cheaply compute noisy gradients by subsampling only a few of these terms.


\subsubsection{Natural gradient ascent} \label{section:natural_gradient_ascent}

Gradient descent becomes problematic when it comes to doing inference in probabilistic models. 

Consider a probabilistic model with a hidden variable $x$ and corresponding parameters $\theta$, with a general objective function $\Lagr(\theta)$. From the definition of a derivative:
\[
	\nabla \Lagr(\theta) = \lim_{||h||\to0} \frac{\Lagr(\theta + h) - \Lagr(\theta)}{||h||}
\]
where $h$ represents an infinitesimally small positive step in the space of $\theta$.\\
To find the direction of steepest descent, one would need to search over all possible directions $d$ in an infinitely small distance $h$, and select the $\hat{d}$ that gives the largest gradient:
\[
\nabla \Lagr(\theta) = \lim_{h\to0} \frac{1}{h}\argmax_{d \, s.t. \|d\|=h} \Lagr(\theta+d) - \Lagr(\theta)
\]
Importantly, this operation requires a distance metric to quantify what a \textit{small} distance $h$ means. In standard gradient descent, this is measured using an Euclidean norm, and the direction of steepest ascent is hence dependent on the Euclidean geometry of the $\theta$ space. Why is this problematic when working with probability distributions?\\
The problem of using an Euclidean distance to optimise parameters of distributions is that it does not consider the uncertainity that underlies probability distributions. A small step from $\theta^{(t)}$ to $\theta^{(t+1)}$ does not guarantee an equivalently small change from $\Lagr(\theta^{(t)})$ to $\Lagr(\theta^{(t+1)})$.\\
To illustrate this, consider the following example of four random variables

\begin{equation}
	\begin{split}
		\psi_1 &\sim \Ndist{0}{5} \\
		\psi_2 &\sim \Ndist{10}{5}
	\end{split}
	\qquad
	\begin{split}
		\psi_3 &\sim \Ndist{0}{1} \\
		\psi_4 &\sim \Ndist{10}{1}
	\end{split}
\end{equation}

Using the Euclidean metric, the distance between $\psi_1$ and $\psi_2$ is the same as the distance between $\psi_3$ and $\psi_4$. However, the distance in distribution space (measured for example by the KL divergence) is much larger between $\psi_1$ and $\psi_2$ than between $\psi_3$ and $\psi_4$ (\Cref{fig:problem_with_Euclidean_distances}).

% \begin{figure}[!h]
% 	\begin{center}
% 		\includegraphics[width=0.65\textwidth]{figures/Euclidean_distance_distributions}
% 		\caption{Illustration of the problem of using Euclidean distances to measure distances between parameters of distributions. In both plots, the red and blue distributions are separated by the same Euclidean distance of 100. Yet, the distance in probability space between the two distributions is intuitively much higher in the right plot.}
% 		\label{fig:problem_with_Euclidean_distances}
% 	\end{center}
% \end{figure}

This basic simulation suggests that replacing the Euclidean distance by the KL divergence as a distance metric may be more appropriate in the context of probabilistic modelling:
\[
	\nabla_{KL} \Lagr(\theta) = \lim_{h\to0} \frac{1}{h}\argmax_{d \, s.t. KL[p_\theta||p_{\theta+d}]=h} \Lagr(\theta+d) - \Lagr(\theta)
\]
The direction of steepest ascent measured by the KL divergence is called the natural gradient \cite{Amari1998,Martens2014}.\\
To find the optimal $\hat{d}_{KL}$, one needs to solve the following optimisation problem:
\begin{equation*} \begin{aligned}
	&\argmin_{d} \Lagr(\theta+d) \qquad
	& \text{subject to}
	& \quad KL[p_\theta||p_{\theta+d}] < c
\end{aligned} \end{equation*}
where $c$ is an arbitrary constant. We will not derive the solution, but this can be solved by introducing Lagrange multipliers and Taylor expansions (see \cite{Amari1998,Kristiadi2019}). The solution corresponds to the standard (Euclidean) gradient pre-multiplied by the inverse of the Fisher Information Matrix of $q(x|\theta)$:
\begin{equation}\label{natural_gradient}
	\hat{d}_{KL} \propto \bfF^{-1}(\theta) \nabla_{\theta} \Lagr(\theta)
\end{equation}
where $\bfF(\theta)$ is defined as
\[
	\bfF(\theta) = \E_{q(x|\theta)}[(\nabla_\theta \log q(x|\theta)) (\nabla_\theta \log q(x|\theta))^T]
\]
%Effectively, the premultiplication by $\bfF^{-1}$ takes into account the local curvate of $q(\theta)$ in distribution space. \\

%Importantly, when $q(x|\theta)$ belongs to the exponential family, the Fisher Information matrix is simply the Hessian of the log normalizer.\\

In conclusion, while the standard gradient points to the direction of steepest ascent in Euclidean space, the natural gradient points to the direction of steepest ascent in a space where distances are defined by the KL divergence \cite{Kristiadi2019,Amari1998,Hoffman2012}.


\subsection{Derivation of a stochastic variational inference algorithm}

In this section I will show how to derive a stochastic variational inference algorithm for general Bayesian models. This work is inspired from \cite{Hoffman2012} which we adapted and implemented in the MOFA model.\\
This section builds upon three theoretical fundations that have been introduced before: Variational inference (\Cref{XXXX}), exponential family distributions (\Cref{section:exponential_family}) and (natural) gradient ascent (\Cref{section:gradient_ascent}).

In this section I will not aim to recount a comprehensive derivation of the algorithm. I will describe a modified and simplified derivation of the main elements that are essential to understand the algorithm. For a complete mathematical derivation we refer the reader to \cite{Hoffman2012}.

% COPIED
%The stochastic nature of the approach is interesting when one dimension of the matrix of observed variables is much larger than the others. In our case, it corresponds to $N$, the number of samples (or cells). \\

\subsubsection{Model definition}

Consider a probabilistic model with a set of unobserved random variables, observations and (non-random) parameters. We begin by classifying the variables of the model into four different categories:

% what about the params for the local variables???
\begin{itemize}
	\itemsep-1.5em
	\item observations ($\bfY$): $N$ different vectors $\bfy_{n}$ which contain the observed variables for the $n$-th sample. \\
	\item local (hidden) variables ($\bfZ)$: $N$ different vectors $\bfz_{n}$ which contain all $K$ hidden variables associated with each sample $n$. \\
	\item global (hidden) variables ($\bbeta$): one vector that contains all $B$ hidden variables not indexed by $n$. \\
	\item parameters ($\balpha$): a vector that contains all fixed parameters for the global variables.
\end{itemize}

This is the corresponding graphical model representation:

\begin{figure}[H]
	\centering	
	\input{graphical_models/general_stochastic}
	\caption{\textbf{Graphical model for a general probabilistic model where unobserved variables are classified as global or local variables.}\\
	}
	\label{fig:graphical_model_stochastic}
\end{figure}

% The first key assumption of this approach is that the prior distributions of the global variables and the local variables are members of the exponential family:
% \begin{align} \label{eq_priors} \begin{split}
% 	p(\beta|\alpha_{\beta}) = h(\beta) \exp\{ \eta_g(\alpha_{\beta}) t(\beta) - a_g(\alpha_{\beta}) \} \\
% 	p(z_{nk}|\alpha_{z}) = h(z_{nk}) \exp\{ \eta_l(\alpha_{z}) t(z_{nk}) - a_l(\alpha_{z}) \}
% \end{split} \end{align}


% COPIED FROM OLD
% The second assumption is that the complete conditionals of the hidden variables are also members of the exponential family:
% \begin{align} \label{eq_complete_conditionals} \begin{split}
% 	p(\beta|\bfY,\bfZ,\balpha) = h(\beta) \exp\{ \eta_g(\bfY,\bfZ,\balpha)^T t(\beta) - a_g(\eta_g(\bfY,\bfZ,\balpha)) \} \\
% 	p(\bfz_{n}|\bfy_{nj},\bfz_{nj},\beta) = h(\bfz_{n}) \exp\{ \eta_l(\bfy_{nj}, \bfz_{nj},\beta)^T t(\bfz_{n}) - a_l(\eta_l(\bfy_{nj},\bfz_{nj},\beta)) \}
% \end{split} \end{align}

% COPIED FROM BIORXIV
% \item the complete conditionals are members of the exponential family:
% \begin{align} \label{eq_complete_conditionals} \begin{split}
% 	p(\beta|\bfY,\bfZ,\balpha) = h(\beta) \exp\{ \eta_g(\bfY,\bfZ,\balpha)^T t(\beta) - a_g(\eta_g(\bfY,\bfZ,\balpha)) \} \\
% 	p(\bfz_{n}|\bfy_{nj},\bfz_{nj},\beta) = h(\bfz_{n}) \exp\{ \eta_l(\bfy_{nj}, \bfz_{nj},\beta)^T t(\bfz_{n}) - a_l(\eta_l(\bfy_{nj},\bfz_{nj},\beta)) \}
% \end{split} \end{align}

\subsubsection{Setting up the inference problem}

% Copoed from Hoffman
% In variational inference, we define a flexible family of distributions over the hidden variables, indexed by free parameters (Jordan et al., 1999; Wainwright and Jordan, 2008). We then find the setting of the parameters (i.e., the member of the family) that is closest to the posterior. Thus we solve the inference problem by solving an optimization problem.\\
%Variational inference is amenable to stochastic optimization because the variational objective decomposes into a sum of terms, one for each data point in the analysis. We can cheaply obtain noisy estimates of the gradient by subsampling the data and computing a scaled gradient on the subsample. If we sample independently then the expectation of this noisy gradient is equal to the true gradient.

%  First, we set up the variational distributions for both the local variables and the global variables. Here we are going to assume that all hidden variables are independent (mean-field assumption)
% \[
% 	q(\bfz,\beta) = q(\beta|\lambda) \prod_{n=1}^{N} \prod_{k=1}^{K} p(z_{nk}|\phi_{nk})
% \]
% and belong to the same exponential family as the corresponding prior distribution:
% \begin{align} \label{eq_variational_distributions}
% 	q(\beta|\lambda) &= h(\beta) \exp\{ \eta_g(\lambda) t(\beta) - a_g(\lambda) \} \\
% 	q(z_{nk}|\phi_{nk}) &= h(z_{nk}) \exp \{ \eta_l(\phi_{n}) t(z_{nk}) - a_l(z_{nk}) \}
% \end{align}
% where $\lambda$ are the parameters governing the global variable and $\phi_{nk}$ are the parameters governing the $k$-th local variable for the $n$-th sample.\\

% From the assumptions above, the ELBO (\Cref{eq_elbo1}) factorises as:
% \begin{align} \label{eq_elbo_factorised} \begin{split}
% 	\Lagr &= \E_{q(\bfZ,\beta)}[\log p(\bfY,\bfZ,\beta)] - \E_{q(\bfZ)}[\log q(\bfZ)] - \E_{q(\beta)}[\log q(\beta)] \\
% 	 &= \sum_{n=1}^{N} \E_{q(\bfz_n,\beta)}[\log p(\bfy_n,\bfz_n,\beta)] - \sum_{n=1}^{N} \sum_{k=1}^{K} \E_{q(z_{nk})}[\log q(z_{nk})] - \E_{q(\beta)}[\log q(\beta)]
% \end{split} \end{align}
% This defines the objetive function. The next step is to derive an iterative algorithm to find the values of the variational parameters that maximise the ELBO.


\subsubsection{Computing the gradient for the global parameters}

To derive the updates for the global parameters we first write the ELBO in terms of $\lambda$:
\[
	\Lagr(\lambda) = \E_{q(z,\beta)}[\log p(\beta|\bfY,\bfZ)] - \E_{q(\beta)}[\log q(\beta)] + \const
\]
where the constant term captures all quantities that do not depend on $\beta$. Then, from the assumption that the complete conditionals and the variational distributions belong to the exponential family (\Crefrange{eq_complete_conditionals}{eq_variational_distributions}):
\baln
	\Lagr(\lambda) &= \E_{q(z,\beta)}[\eta_g(\bfY,\bfZ,\balpha)^T t(\beta)] - \E_{q(\beta)}[\lambda^T t(\beta) - a_g(\lambda) ] + \const \\
	&= \E_{q(z)}[\eta_g(\bfY,\bfZ,\balpha)^T] \nabla a(\lambda) - \lambda^T \nabla a_g(\lambda) - a_g(\lambda) + \const
\ealn
where we have used the exponential family identity $\E_{q(\beta)}[t(\beta)] = \nabla a_g(\lambda)$. \\
Taking the gradient with respect to $\lambda$:
\begin{equation} \label{gradient_global}
	\nabla_{\lambda} \Lagr(\lambda) = \nabla_{\lambda}^{2} a_g(\lambda)(\E_{q(z)}[\eta_g(\bfY,\bfZ,\balpha)] - \lambda)
\end{equation}
and setting it to zero leads to the solution:
\begin{equation} \label{solution_global}
	\lambda = \E_{q(z)}[\eta_g(\bfY,\bfZ,\balpha)]
\end{equation}
Hence, we conclude that the global variational parameter $\lambda$ corresponds to the expectation (with respect to $q(z)$) of the natural parameter $\eta$ of the complete conditional for $\beta$.